{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "# If below code does not work, copy and paste this code in the terminal: huggingface-cli login \n",
    "# then paste this read token: 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# It may not show up but still paste the token in and press enter\n",
    "\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment based on if you want to do EVENT, CAUSE, or, ACTION\n",
    "# training_label = 'EVENT'\n",
    "training_label = 'CAUSE'\n",
    "# training_label = 'ACTION'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code you should currently use. Just copy and paste the text in here and will will spit out a demo answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# set up the pipeline\n",
    "classifier = pipeline(\"text-classification\", model=f\"Chantland/HRAF_{training_label}_Demo\", use_auth_token=\"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\", tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'PRESENT', 'score': 0.9589055180549622}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample inference ENTER TEXT IN HERE.\n",
    "text = \"What is meant by the flight to the Milky Way? We are told that behind the visible Milky Way there is another Milky Way, and this is the one the sorcerer visits in his dreams. It is like a kuntanka (tjurunga) and the medicine man walks along it. It is beautiful and smooth like the subincision opening. If the subincision opening were not beautiful and smooth the medicine man might stick there and his soul might never go back into his body. (52) Ibid., pp. 69, 70. ‚Äù\"\n",
    "tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}\n",
    "\n",
    "classifier(text, **tokenizer_kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datset <font color=\"red\">(do not run more than 1 cell)</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old Dataset (dataset this was trained on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['passage', 'label'],\n",
       "    num_rows: 1750\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# load the dataset (THIS IS WHERE YOU WOULD ENTER IN THE DATA YOU WANTED TO TEST!)\n",
    "df = pd.read_excel(\"../RA_Cleaning/Culture_Coding_old.xlsx\", header=[0,1], index_col=0)\n",
    "# subdivide into just passage and outcome\n",
    "df_small = pd.DataFrame()\n",
    "df_small[[\"passage\",\"label\"]] = df[[('CULTURE', \"Passage\"), (training_label, \"No_Info\")]]\n",
    "# Flip the lable of \"no_info\"\n",
    "df_small[\"label\"] = df_small['label'].replace({0:1, 1:0})\n",
    "\n",
    "\n",
    "\n",
    "# Create an NLP friendly dataset\n",
    "Hraf = Dataset.from_dict(df_small.to_dict(orient= 'list'))\n",
    "Hraf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Codings: (Current Dataset - MINUS - Old Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['passage', 'label'],\n",
       "    num_rows: 140\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load OLD and new datasets (THIS IS WHERE YOU WOULD ENTER IN THE DATA YOU WANTED TO TEST!)\n",
    "df_old = pd.read_excel(\"../RA_Cleaning/Culture_Coding_old.xlsx\", header=[0,1], index_col=0)\n",
    "df_current = pd.read_excel(\"../RA_Cleaning/Culture_Coding.xlsx\", header=[0,1], index_col=0)\n",
    "\n",
    "# Remove the runs not of the first from the datasets (this will be made superfluous later but nonetheless is an extra assuredness step)\n",
    "# df_old = df_old.loc[df_old[(\"CODER\",\"Run_Number\")]==1] #if this had \"Run_Number\" column, you would uncomment and run this line\n",
    "df_current = df_current.loc[df_current[(\"CODER\",\"Run_Number\")]==1]\n",
    "\n",
    "# only get new rows that have NOT been trained/tested on before\n",
    "df_new = pd.concat([df_current, df_old])\n",
    "df_new = df_new[~df_new.duplicated(subset=(\"CULTURE\",\"Passage Number\"), keep=False)]\n",
    "\n",
    "\n",
    "# subdivide into just passage and outcome\n",
    "df_small = pd.DataFrame()\n",
    "df_small[[\"passage\",\"label\"]] = df_new[[('CULTURE', \"Passage\"), (training_label, \"No_Info\")]]\n",
    "# Flip the lable of \"no_info\"\n",
    "df_small[\"label\"] = df_small['label'].replace({0:1, 1:0})\n",
    "\n",
    "\n",
    "\n",
    "# Create an NLP friendly dataset\n",
    "Hraf = Dataset.from_dict(df_small.to_dict(orient= 'list'))\n",
    "Hraf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of passages and predict them (will take about .25 seconds per passage for me so beware the wait)\n",
    "\n",
    "HrafOutput = []\n",
    "for text in Hraf:\n",
    "    # text = Hraf[text]\n",
    "    score = classifier(text['passage'], **tokenizer_kwargs)\n",
    "    score[0][\"actual_label\"] = text['label']\n",
    "    score[0][\"passage\"] = text['passage']\n",
    "\n",
    "    # change the predicted label into a digit that we can compare\n",
    "    if score[0]['label'] == 'PRESENT':\n",
    "        score[0]['label'] = 1\n",
    "    elif score[0]['label'] == 'ABSENT':\n",
    "        score[0]['label'] = 0\n",
    "    else:\n",
    "        score[0]['label'] = 9\n",
    "\n",
    "\n",
    "    # score[0][(\"actual_label\", 'passage')] = text['passage'], text['label']\n",
    "    HrafOutput.append(score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HrafOutput"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate \"Correctness\" Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.862\n",
      "F1 score (base)  0.907\n",
      "F1 score (micro) 0.862\n",
      "F1 score (macro) 0.819\n"
     ]
    }
   ],
   "source": [
    "# Try preconstructed evaluations\n",
    "\n",
    "# loading precision\n",
    "precision = evaluate.load('precision')\n",
    "result = precision.compute(predictions=[x['label'] for x in HrafOutput], references=[x['actual_label'] for x in HrafOutput])\n",
    "# print(result)\n",
    "\n",
    "# f1 score\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "accuracy = round(accuracy_score([x['actual_label'] for x in HrafOutput], [x['label'] for x in HrafOutput]),3)\n",
    "f1 = round(f1_score([x['actual_label'] for x in HrafOutput], [x['label'] for x in HrafOutput], average = 'binary'),3)\n",
    "f1_micro = round(f1_score([x['actual_label'] for x in HrafOutput], [x['label'] for x in HrafOutput], average='micro'),3)\n",
    "f1_macro = round(f1_score([x['actual_label'] for x in HrafOutput], [x['label'] for x in HrafOutput], average='macro'),3)\n",
    "print(f'Accuracy  {accuracy}\\nF1 score (base)  {f1}\\nF1 score (micro) {f1_micro}\\nF1 score (macro) {f1_macro}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Correctness Metric Myself\n",
    "This was made before finding out we could just load preconstructed, but it is great for the confusion matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up confusion matrix for calculating precision myself\n",
    "confusionMatrix_dict = {\"TruePos\":0, \"FalsePos\":0, \"FalseNeg\":0, \"TrueNeg\":0}\n",
    "for text in HrafOutput:\n",
    "    if text['actual_label'] == 1:\n",
    "        if text['label'] == 1:\n",
    "            confusionMatrix_dict[\"TruePos\"] += 1\n",
    "        elif text['label'] == 0:\n",
    "            confusionMatrix_dict[\"FalseNeg\"] += 1\n",
    "        else:\n",
    "            raise Exception(\"ERROR pos\")\n",
    "    elif text['actual_label'] == 0:\n",
    "        if text['label'] == 1:\n",
    "            confusionMatrix_dict[\"FalsePos\"] += 1\n",
    "        elif text['label'] == 0:\n",
    "            confusionMatrix_dict[\"TrueNeg\"] += 1\n",
    "        else:\n",
    "            raise Exception(\"ERROR neg\")\n",
    "    else:\n",
    "        raise Exception(\"ERROR actual\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TruePos': 68, 'FalsePos': 28, 'FalseNeg': 2, 'TrueNeg': 42}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusionMatrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusionMatrix_dict['TruePos']\n",
    "TN = confusionMatrix_dict['TrueNeg']\n",
    "FP = confusionMatrix_dict['FalsePos']\n",
    "FN = confusionMatrix_dict['FalseNeg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:   0.708\n",
      "Recall:      0.971\n",
      "F1:          0.819\n",
      "MCC:         0.728\n"
     ]
    }
   ],
   "source": [
    "# get scores \n",
    "precision = TP/ (TP + FP)\n",
    "recall = TP/ (TP + FN)\n",
    "F1 = (2 * precision * recall) / (precision + recall)\n",
    "F1_test = TP/ (TP + .5*(FP + FN)) #done to double check work\n",
    "MCC_num = (TP * TN) - (FP - FN)\n",
    "MCC_denom = np.sqrt((TP + FN) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "MCC = MCC_num / MCC_denom #mathews correlation coefficient\n",
    "\n",
    "assert round(F1,5) == round(F1_test,5), ValueError\n",
    "print(f'Precision:   {round(precision,3)}\\nRecall:      {round(recall,3)}\\nF1:          {round(F1,3)}\\nMCC:         {round(MCC,3)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quick data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional File save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally save the file to json\n",
    "from transformers import AutoTokenizer\n",
    "import copy\n",
    "\n",
    "HrafOutput_dummy = copy.deepcopy(HrafOutput)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"passage\"], truncation=True)\n",
    "\n",
    "tokenized_Hraf = Hraf.map(preprocess_function, batched=True)\n",
    "\n",
    "for index, passage in enumerate(HrafOutput_dummy):\n",
    "    assert passage['passage'] == tokenized_Hraf[index]['passage']\n",
    "    passage['predicted_label'] = passage.pop('label') # rename label\n",
    "    passage['actual_label'] = tokenized_Hraf[index]['label']\n",
    "    passage['input_ids'] = tokenized_Hraf[index]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HrafOutput_dummy = Dataset.from_list(HrafOutput_dummy)\n",
    "Dataset.to_json(HrafOutput_dummy, f\"../Tokenized_Datasets/tokenized_Hraf_{training_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
