{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color=\"red\">NOTE</font> This code is outdated and a new model shall be created. Culture_Coding.xlsx files have been changed to _Altogether_Dataset_RACoded.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e35182bde84894bec8b46e06cc226d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "# If below code does not work, copy and paste this code in the terminal: huggingface-cli login \n",
    "# then paste this read token: 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# It may not show up but still paste the token in and press enter\n",
    "\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of passages and predict them (will take about .25 seconds per passage for me so beware the wait)\n",
    "def predictor(data, labels, tokenizer_kwargs, classifier):\n",
    "    dataOutput = []\n",
    "    for text in data:\n",
    "        # get actual labels\n",
    "        actual_labels = [text[label] for label in labels]\n",
    "        prediction = classifier(text['passage'], **tokenizer_kwargs)\n",
    "\n",
    "        # get predicted labels\n",
    "        scores = {item['label']:item['score'] for item in prediction[0]} #turn prediction into a dictionary\n",
    "        pred_labels = [1 if scores[label] >= 0.5 else 0 for label in labels]\n",
    "\n",
    "        \n",
    "        output_dict = dict()\n",
    "        output_dict[\"pred_labels\"] = pred_labels\n",
    "        output_dict[\"actual_labels\"] = actual_labels\n",
    "        output_dict[\"passage\"] = text['passage']\n",
    "        output_dict[\"ID\"] = text['ID']\n",
    "\n",
    "\n",
    "        # score[0][(\"actual_label\", 'passage')] = text['passage'], text['label']\n",
    "        dataOutput.append(output_dict)\n",
    "    return dataOutput\n",
    "\n",
    "# Get F1 scores\n",
    "def score(dataOutput, labels):\n",
    "    from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "    df_score = pd.DataFrame(index=['NLP'], columns= [label+\"_F1\" for label in labels] + [\"Micro_F1\", \"Macro_F1\"])\n",
    "    actual_labels = [x['actual_labels'] for x in dataOutput]\n",
    "    pred_labels = [x['pred_labels'] for x in dataOutput]\n",
    "    for index, label in enumerate(labels):\n",
    "        f1 = round(f1_score(y_true=np.array(actual_labels)[:,index], y_pred=np.array(pred_labels)[:,index]),3)\n",
    "        df_score.at['NLP', label+\"_F1\"] = f1\n",
    "        # print(f\"{label}: {(6 - len(label)) *' '}{f1}\")\n",
    "\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    f1_micro = round(f1_score(y_true=actual_labels, y_pred=pred_labels, average='micro'),3)\n",
    "    f1_macro = round(f1_score(y_true=actual_labels, y_pred=pred_labels, average='macro'),3)\n",
    "    df_score.at['NLP', \"Micro_F1\"] = f1_micro\n",
    "    df_score.at['NLP', \"Macro_F1\"] = f1_macro\n",
    "    return df_score\n",
    "    # print(f'F1 score (micro) {f1_micro}\\nF1 score (macro) {f1_macro}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'passage', 'EVENT', 'CAUSE', 'ACTION'],\n",
       "    num_rows: 1895\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "loc = \"\"\n",
    "# loc = \"../HRAF_MultiLabel_ThreeLargeClasses/\" #load old threemain class (comment this out unless you specifically are using it)\n",
    "\n",
    "f = open(loc+\"Datasets/test_dataset.json\")\n",
    "# f = open(\"../HRAF_MultiLabel_ThreeLargeClasses/Datasets/test_dataset.json\") #load old threemain class (comment this out unless you specifically are using it)\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "Hraf = Dataset.from_dict(data)\n",
    "Hraf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Kwargs and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EVENT', 'CAUSE', 'ACTION']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define tokenizer kwargs\n",
    "tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}\n",
    "\n",
    "classifier_kwargs = {'top_k':None, 'device':0} #Set device -1 for CPU, 0 or higher for GPU\n",
    "\n",
    "# get label names\n",
    "labels = [label for label in Hraf.features.keys() if label not in ['ID', 'passage']]\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this or the other model, not both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'EVENT', 'score': 0.9999905824661255},\n",
       "  {'label': 'ACTION', 'score': 0.9999289512634277},\n",
       "  {'label': 'CAUSE', 'score': 1.0256605492031667e-05}]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# CHANGE Model name\n",
    "model = \"MultiLabel_ThreeLargeClasses_kfoldsDEMO_Spring2024\"\n",
    "\n",
    "# set up the pipeline from local\n",
    "import os\n",
    "path =os.path.abspath(f\"HRAF_Model_{model}\")\n",
    "classifier = pipeline(\"text-classification\", model=path, **classifier_kwargs)\n",
    "\n",
    "\n",
    "# sample inference ENTER TEXT IN HERE.\n",
    "text = '''\n",
    "“Drinking-tubes made of the leg-bones of swans (Fig. 109) are 190 also used chiefly as a measure of precaution against diseases ‘subject to shunning.’....”\n",
    "'''\n",
    "# reveal sample classification\n",
    "prediction = classifier(text, **tokenizer_kwargs)\n",
    "prediction\n",
    "\n",
    "# # Demo other models (COMMENT THIS OUT UNLESS YOU REALLY WANT TO DEMO THIS)\n",
    "# # Set up path from online hub (note, this is analogous but different model and is here because this is a demo)\n",
    "# classifier = pipeline(\"text-classification\", top_k=None, model=\"Chantland/Hraf_MultiLabel\", use_auth_token=\"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\", tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\"))\n",
    "# model = \"MultiLabel_ThreeLargeClasses\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1895 passages Predicted\n"
     ]
    }
   ],
   "source": [
    "# Predict dataset (may take about .25 seconds per passage when tested on lab mac, could differ depending on your system)\n",
    "# Also note that this pipeline is sequential and may give a warning saying it is unoptimized. Currently, using a whole dataset does not seem to reap faster results so we are remaining with sequential\n",
    "HrafOutput = predictor(Hraf, labels=labels, tokenizer_kwargs=tokenizer_kwargs, classifier=classifier)\n",
    "print(len(HrafOutput), \"passages Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HrafOutput = classifier(Hraf['passage'],**tokenizer_kwargs)\n",
    "# print(len(HrafOutput), \"passages Predicted\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate \"Correctness\" Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get F1 scores for labels.\n",
    "df_score = score(HrafOutput, labels)\n",
    "df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add correctness to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>EVENT_F1</th>\n",
       "      <th>CAUSE_F1</th>\n",
       "      <th>ACTION_F1</th>\n",
       "      <th>Micro_F1</th>\n",
       "      <th>Macro_F1</th>\n",
       "      <th>test_length</th>\n",
       "      <th>train_length</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>2024-04-04</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.845</td>\n",
       "      <td>1820</td>\n",
       "      <td>7277</td>\n",
       "      <td>model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>2024-02-22</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1085</td>\n",
       "      <td>4340</td>\n",
       "      <td>model: MultiLabel_ThreeLargeClasses, Dataset: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>2024-02-22</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1085</td>\n",
       "      <td>4340</td>\n",
       "      <td>model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.847</td>\n",
       "      <td>1123</td>\n",
       "      <td>4491</td>\n",
       "      <td>model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>2024-02-15</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.846</td>\n",
       "      <td>291</td>\n",
       "      <td>1648</td>\n",
       "      <td>model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lexical search</th>\n",
       "      <td>2024-02-02</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.553</td>\n",
       "      <td>728</td>\n",
       "      <td>2910</td>\n",
       "      <td>Ngram 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>2024-02-02</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.809</td>\n",
       "      <td>728</td>\n",
       "      <td>2910</td>\n",
       "      <td>model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Date EVENT_F1 CAUSE_F1 ACTION_F1 Micro_F1 Macro_F1   \n",
       "NLP            2024-04-04    0.907    0.822     0.805    0.851    0.845  \\\n",
       "NLP            2024-02-22    0.871     0.81     0.793    0.828    0.824   \n",
       "NLP            2024-02-22    0.916    0.827     0.838    0.865     0.86   \n",
       "NLP            2024-02-16    0.906     0.82     0.813    0.851    0.847   \n",
       "NLP            2024-02-15    0.939    0.828      0.77    0.853    0.846   \n",
       "Lexical search 2024-02-02     0.82    0.738     0.656     0.75    0.553   \n",
       "NLP            2024-02-02    0.883    0.812     0.733    0.816    0.809   \n",
       "\n",
       "                test_length  train_length   \n",
       "NLP                    1820          7277  \\\n",
       "NLP                    1085          4340   \n",
       "NLP                    1085          4340   \n",
       "NLP                    1123          4491   \n",
       "NLP                     291          1648   \n",
       "Lexical search          728          2910   \n",
       "NLP                     728          2910   \n",
       "\n",
       "                                                            Notes  \n",
       "NLP             model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...  \n",
       "NLP             model: MultiLabel_ThreeLargeClasses, Dataset: ...  \n",
       "NLP             model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...  \n",
       "NLP             model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...  \n",
       "NLP             model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...  \n",
       "Lexical search                                            Ngram 1  \n",
       "NLP             model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# export F1 scores to excel\n",
    "df_scoresSep = df_score.copy()\n",
    "# first load train (and maybe add validation)\n",
    "f = open(loc+\"Datasets/train_dataset.json\")\n",
    "data = json.load(f)\n",
    "train = Dataset.from_dict(data)\n",
    "if os.path.isfile(loc+\"Datasets/validation_dataset.json\"):\n",
    "    f = open(loc+\"Datasets/validation_dataset.json\")\n",
    "    data = json.load(f)\n",
    "    valid = Dataset.from_dict(data)\n",
    "    train = concatenate_datasets([train, valid])\n",
    "# add lengths of test and training set\n",
    "df_scoresSep[[\"test_length\", \"train_length\"]] = (len(Hraf), len(train))\n",
    "# add date\n",
    "df_scoresSep.insert(0, \"Date\", [datetime.today().date()])\n",
    "if loc == \"\":\n",
    "    df_scoresSep['Notes'] = f\"model: {model}, Dataset: {model}\"\n",
    "else:\n",
    "    df_scoresSep['Notes'] = f\"model: {model}, Dataset: {loc}\"\n",
    "# load model_performance.xlsx or else create it\n",
    "if os.path.isfile(\"Model_Prediction_Performance.xlsx\"):\n",
    "    df_oldScores = pd.read_excel(\"Model_Prediction_Performance.xlsx\", index_col=0)\n",
    "    df_oldScores_merged = pd.concat([df_scoresSep, df_oldScores])\n",
    "    nonDateCols = df_oldScores_merged.columns[df_scoresSep.columns != 'Date']\n",
    "    if any(df_oldScores_merged.duplicated(subset=nonDateCols)): # don't append the data unless it is new\n",
    "        print(\"Duplicated scores found, skipping new addition\")\n",
    "        df_scoresSep = df_oldScores.copy()\n",
    "    else:\n",
    "        df_scoresSep = df_oldScores_merged.copy()\n",
    "        df_scoresSep['Date'] = df_scoresSep['Date'].astype('datetime64[ns]')\n",
    "        df_scoresSep.to_excel(\"Model_Prediction_Performance.xlsx\")\n",
    "else:\n",
    "    df_scoresSep['Date'] = df_scoresSep['Date'].astype('datetime64[ns]')\n",
    "    df_scoresSep.to_excel(\"Model_Prediction_Performance.xlsx\")\n",
    "df_scoresSep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Multi-model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to run over MANY models and checkpoints to test and see which is the strongest. This is ran instead of the single model one above and should NOT be ran together with the single model (simply because they do different things)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for running through all checkpoints\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "def checkpointInfer(path, data, labels, tokenizer_kwargs, classifier_kwargs, folds=True, output_str=\"output_dir_\", modelDestinctifier:str= \"ModelDistinctifierUnknown\"):\n",
    "    # Initiate Dataframe overall\n",
    "    df = pd.DataFrame([])\n",
    "\n",
    "    # Get all viable models \n",
    "    # Makes sure the model starts with the output string and is a directory\n",
    "    models = [name for name in os.listdir(path) if (name.startswith(output_str) and os.path.isdir(f\"{path}/{name}\"))]\n",
    "\n",
    "    for model in models:\n",
    "        # Initiate Dataframe for each model\n",
    "        df_model = pd.DataFrame([])\n",
    "\n",
    "        checkpoints_dir = [checkpoint for checkpoint in os.listdir(f\"{path}/{model}\") if checkpoint.startswith(\"checkpoint\")] \n",
    "\n",
    "        modelDestinctifier_unit = re.findall(f\"{output_str}(.*?)_\",model)\n",
    "        try:\n",
    "            modelDestinctifier_unit = float(modelDestinctifier_unit[0])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "\n",
    "        for checkpoint in checkpoints_dir:\n",
    "            # Initiate Dataframe for each checkpoint\n",
    "            df_checkpoint = pd.DataFrame([])\n",
    "            # set up the pipeline from local\n",
    "            model_path =os.path.abspath(f\"{path}/{model}/{checkpoint}\")\n",
    "            classifier = pipeline(\"text-classification\", model=model_path, **classifier_kwargs)\n",
    "            # Get Predictions\n",
    "            dataOutput = predictor(data, labels=labels, tokenizer_kwargs=tokenizer_kwargs, classifier=classifier)\n",
    "            # Get scores\n",
    "            df_checkpoint = score(dataOutput, labels)\n",
    "            df_checkpoint = df_checkpoint.reset_index(drop=True) #remove the index here\n",
    "\n",
    "            \n",
    "            df_checkpoint.insert(0,modelDestinctifier,modelDestinctifier_unit) #insert model distinctifier (like weight decay or learning rate)\n",
    "            #Extract and add Fold name if relevant\n",
    "            if folds: #if using folds\n",
    "                fold = re.findall(r\"fold_(\\d*)\",model)\n",
    "                fold = int(fold[0])\n",
    "                df_checkpoint.insert(1,\"Fold\",fold)\n",
    "            else:\n",
    "                fold = \"\"\n",
    "\n",
    "            # get checkpoint\n",
    "            checkpoint_num = re.findall(r\"checkpoint-(\\d*)\",checkpoint)\n",
    "            assert len(checkpoint_num) == 1, f\"More or less than one checkpoint numbers found: {len(checkpoint_num)} checkpoints\"\n",
    "            checkpoint_num = int(checkpoint_num[0])\n",
    "\n",
    "            df_checkpoint.insert(0,\"Checkpoint\",checkpoint_num)\n",
    "            df_model = pd.concat([df_model,df_checkpoint])\n",
    "            print(model, checkpoint, \"Complete\")\n",
    "\n",
    "        # concat model to overarching dataframe\n",
    "        df = pd.concat([df,df_model])\n",
    "        # save df for each model (as a checkpoint)\n",
    "        # import evaluation if it exists\n",
    "        if os.path.exists(f\"{path}/Inference_Test.xlsx\"):\n",
    "            old_df = pd.read_excel(f\"{path}/Inference_Test.xlsx\", sheet_name=\"Sheet1\", index_col=0)\n",
    "            df_model = pd.concat([old_df, df_model])\n",
    "\n",
    "        df_model.to_excel(f\"{path}/Inference_Test.xlsx\", sheet_name=\"Sheet1\")\n",
    "        print(model, \"Successfully Saved\")\n",
    "\n",
    "    return df\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# output_str=\"output_dir_\"\n",
    "\n",
    "# model = \"MultiLabel_ThreeLargeClasses_kfoldsDEMO_WeightInvestigation\"\n",
    "# path =os.path.abspath(f\"HRAF_Model_{model}\")\n",
    "# x = [name for name in os.listdir(path) if (name.startswith(\"output_dir_\") and os.path.isdir(f\"{path}/{name}\"))]\n",
    "# # x\n",
    "# modelDestinctifier_unit = re.findall(f\"{output_str}(.*?)_\",x[1])\n",
    "# try:\n",
    "#     modelDestinctifier_unit = float(modelDestinctifier_unit)\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#This code will take a LONG time depending on how many models you have. It is reccommended to use a GPU\u001b[39;00m\n\u001b[1;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHRAF_Model_MultiLabel_ThreeLargeClasses_kfoldsDEMO_WeightInvestigation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m df_allScores \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpointInfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHraf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mclassifier_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_dir_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelDestinctifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWeight_Decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m df_allScores\n",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m, in \u001b[0;36mcheckpointInfer\u001b[0;34m(path, data, labels, tokenizer_kwargs, classifier_kwargs, folds, output_str, modelDestinctifier)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Initiate Dataframe for each model\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     df_model \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([])\n\u001b[0;32m---> 19\u001b[0m     checkpoints_dir \u001b[38;5;241m=\u001b[39m [checkpoint \u001b[38;5;28;01mfor\u001b[39;00m checkpoint \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m checkpoint\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)] \n\u001b[1;32m     21\u001b[0m     modelDestinctifier_unit \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(.*?)_\u001b[39m\u001b[38;5;124m\"\u001b[39m,model)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-4/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:1197\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_line:\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_suspend(thread, step_cmd, original_step_cmd\u001b[38;5;241m=\u001b[39minfo\u001b[38;5;241m.\u001b[39mpydev_original_step_cmd)\n\u001b[0;32m-> 1197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_return:  \u001b[38;5;66;03m# return event\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m     back \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_back\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-4/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-4/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-4/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#This code will take a LONG time depending on how many models you have. It is reccommended to use a GPU\n",
    "path = \"HRAF_Model_MultiLabel_ThreeLargeClasses_kfoldsDEMO_WeightInvestigation\"\n",
    "\n",
    "df_allScores = checkpointInfer(path=path, data=Hraf, labels=labels, tokenizer_kwargs=tokenizer_kwargs,  classifier_kwargs=classifier_kwargs, folds=True, output_str=\"output_dir_\", modelDestinctifier= \"Weight_Decay\")\n",
    "df_allScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir_0.0001_fold_1 checkpoint-1516 Complete\n",
      "output_dir_0.0001_fold_1 checkpoint-2274 Complete\n",
      "output_dir_0.0001_fold_1 checkpoint-3032 Complete\n",
      "output_dir_0.0001_fold_1 checkpoint-3790 Complete\n",
      "output_dir_0.0001_fold_1 checkpoint-758 Complete\n",
      "output_dir_0.0001_fold_1 Successfully Saved\n",
      "output_dir_0.0001_fold_2 checkpoint-1516 Complete\n",
      "output_dir_0.0001_fold_2 checkpoint-2274 Complete\n",
      "output_dir_0.0001_fold_2 checkpoint-3032 Complete\n",
      "output_dir_0.0001_fold_2 checkpoint-3790 Complete\n",
      "output_dir_0.0001_fold_2 checkpoint-758 Complete\n",
      "output_dir_0.0001_fold_2 Successfully Saved\n",
      "output_dir_0.0001_fold_3 checkpoint-1516 Complete\n",
      "output_dir_0.0001_fold_3 checkpoint-2274 Complete\n",
      "output_dir_0.0001_fold_3 checkpoint-3032 Complete\n",
      "output_dir_0.0001_fold_3 checkpoint-3790 Complete\n",
      "output_dir_0.0001_fold_3 checkpoint-758 Complete\n",
      "output_dir_0.0001_fold_3 Successfully Saved\n",
      "output_dir_0.0001_fold_4 checkpoint-1516 Complete\n",
      "output_dir_0.0001_fold_4 checkpoint-2274 Complete\n",
      "output_dir_0.0001_fold_4 checkpoint-3032 Complete\n",
      "output_dir_0.0001_fold_4 checkpoint-3790 Complete\n",
      "output_dir_0.0001_fold_4 checkpoint-758 Complete\n",
      "output_dir_0.0001_fold_4 Successfully Saved\n",
      "output_dir_0.0001_fold_5 checkpoint-1516 Complete\n",
      "output_dir_0.0001_fold_5 checkpoint-2274 Complete\n",
      "output_dir_0.0001_fold_5 checkpoint-3032 Complete\n",
      "output_dir_0.0001_fold_5 checkpoint-3790 Complete\n",
      "output_dir_0.0001_fold_5 checkpoint-758 Complete\n",
      "output_dir_0.0001_fold_5 Successfully Saved\n",
      "output_dir_0.001_fold_1 checkpoint-1516 Complete\n",
      "output_dir_0.001_fold_1 checkpoint-2274 Complete\n",
      "output_dir_0.001_fold_1 checkpoint-3032 Complete\n",
      "output_dir_0.001_fold_1 checkpoint-3790 Complete\n",
      "output_dir_0.001_fold_1 checkpoint-758 Complete\n",
      "output_dir_0.001_fold_1 Successfully Saved\n",
      "output_dir_0.001_fold_2 checkpoint-1516 Complete\n",
      "output_dir_0.001_fold_2 checkpoint-2274 Complete\n",
      "output_dir_0.001_fold_2 checkpoint-3032 Complete\n",
      "output_dir_0.001_fold_2 checkpoint-3790 Complete\n",
      "output_dir_0.001_fold_2 checkpoint-758 Complete\n",
      "output_dir_0.001_fold_2 Successfully Saved\n",
      "output_dir_0.001_fold_3 checkpoint-1516 Complete\n",
      "output_dir_0.001_fold_3 checkpoint-2274 Complete\n",
      "output_dir_0.001_fold_3 checkpoint-3032 Complete\n",
      "output_dir_0.001_fold_3 checkpoint-3790 Complete\n",
      "output_dir_0.001_fold_3 checkpoint-758 Complete\n",
      "output_dir_0.001_fold_3 Successfully Saved\n",
      "output_dir_0.001_fold_4 checkpoint-1516 Complete\n",
      "output_dir_0.001_fold_4 checkpoint-2274 Complete\n",
      "output_dir_0.001_fold_4 checkpoint-3032 Complete\n",
      "output_dir_0.001_fold_4 checkpoint-3790 Complete\n",
      "output_dir_0.001_fold_4 checkpoint-758 Complete\n",
      "output_dir_0.001_fold_4 Successfully Saved\n",
      "output_dir_0.001_fold_5 checkpoint-1516 Complete\n",
      "output_dir_0.001_fold_5 checkpoint-2274 Complete\n",
      "output_dir_0.001_fold_5 checkpoint-3032 Complete\n",
      "output_dir_0.001_fold_5 checkpoint-3790 Complete\n",
      "output_dir_0.001_fold_5 checkpoint-758 Complete\n",
      "output_dir_0.001_fold_5 Successfully Saved\n",
      "output_dir_0.01_fold_1 checkpoint-1516 Complete\n",
      "output_dir_0.01_fold_1 checkpoint-2274 Complete\n",
      "output_dir_0.01_fold_1 checkpoint-3032 Complete\n",
      "output_dir_0.01_fold_1 checkpoint-3790 Complete\n",
      "output_dir_0.01_fold_1 checkpoint-758 Complete\n",
      "output_dir_0.01_fold_1 Successfully Saved\n",
      "output_dir_0.01_fold_2 checkpoint-1516 Complete\n",
      "output_dir_0.01_fold_2 checkpoint-2274 Complete\n",
      "output_dir_0.01_fold_2 checkpoint-3032 Complete\n",
      "output_dir_0.01_fold_2 checkpoint-3790 Complete\n",
      "output_dir_0.01_fold_2 checkpoint-758 Complete\n",
      "output_dir_0.01_fold_2 Successfully Saved\n",
      "output_dir_0.01_fold_3 checkpoint-1516 Complete\n",
      "output_dir_0.01_fold_3 checkpoint-2274 Complete\n",
      "output_dir_0.01_fold_3 checkpoint-3032 Complete\n",
      "output_dir_0.01_fold_3 checkpoint-3790 Complete\n",
      "output_dir_0.01_fold_3 checkpoint-758 Complete\n",
      "output_dir_0.01_fold_3 Successfully Saved\n",
      "output_dir_0.01_fold_4 checkpoint-1516 Complete\n",
      "output_dir_0.01_fold_4 checkpoint-2274 Complete\n",
      "output_dir_0.01_fold_4 checkpoint-3032 Complete\n",
      "output_dir_0.01_fold_4 checkpoint-3790 Complete\n",
      "output_dir_0.01_fold_4 checkpoint-758 Complete\n",
      "output_dir_0.01_fold_4 Successfully Saved\n",
      "output_dir_0.01_fold_5 checkpoint-1516 Complete\n",
      "output_dir_0.01_fold_5 checkpoint-2274 Complete\n",
      "output_dir_0.01_fold_5 checkpoint-3032 Complete\n",
      "output_dir_0.01_fold_5 checkpoint-3790 Complete\n",
      "output_dir_0.01_fold_5 checkpoint-758 Complete\n",
      "output_dir_0.01_fold_5 Successfully Saved\n",
      "output_dir_1e-05_fold_1 checkpoint-1456 Complete\n",
      "output_dir_1e-05_fold_1 checkpoint-2184 Complete\n",
      "output_dir_1e-05_fold_1 checkpoint-2912 Complete\n",
      "output_dir_1e-05_fold_1 checkpoint-3640 Complete\n",
      "output_dir_1e-05_fold_1 checkpoint-728 Complete\n",
      "output_dir_1e-05_fold_1 Successfully Saved\n",
      "output_dir_1e-05_fold_2 checkpoint-1456 Complete\n",
      "output_dir_1e-05_fold_2 checkpoint-2184 Complete\n",
      "output_dir_1e-05_fold_2 checkpoint-2912 Complete\n",
      "output_dir_1e-05_fold_2 checkpoint-3640 Complete\n",
      "output_dir_1e-05_fold_2 checkpoint-728 Complete\n",
      "output_dir_1e-05_fold_2 Successfully Saved\n",
      "output_dir_1e-05_fold_3 checkpoint-1456 Complete\n",
      "output_dir_1e-05_fold_3 checkpoint-2184 Complete\n",
      "output_dir_1e-05_fold_3 checkpoint-2912 Complete\n",
      "output_dir_1e-05_fold_3 checkpoint-3640 Complete\n",
      "output_dir_1e-05_fold_3 checkpoint-728 Complete\n",
      "output_dir_1e-05_fold_3 Successfully Saved\n",
      "output_dir_1e-05_fold_4 checkpoint-1516 Complete\n",
      "output_dir_1e-05_fold_4 checkpoint-2274 Complete\n",
      "output_dir_1e-05_fold_4 checkpoint-3032 Complete\n",
      "output_dir_1e-05_fold_4 checkpoint-3790 Complete\n",
      "output_dir_1e-05_fold_4 checkpoint-758 Complete\n",
      "output_dir_1e-05_fold_4 Successfully Saved\n",
      "output_dir_1e-05_fold_5 checkpoint-1516 Complete\n",
      "output_dir_1e-05_fold_5 checkpoint-2274 Complete\n",
      "output_dir_1e-05_fold_5 checkpoint-3032 Complete\n",
      "output_dir_1e-05_fold_5 checkpoint-3790 Complete\n",
      "output_dir_1e-05_fold_5 checkpoint-758 Complete\n",
      "output_dir_1e-05_fold_5 Successfully Saved\n",
      "output_dir_1e-06_fold_1 checkpoint-1456 Complete\n",
      "output_dir_1e-06_fold_1 checkpoint-2184 Complete\n",
      "output_dir_1e-06_fold_1 checkpoint-2912 Complete\n",
      "output_dir_1e-06_fold_1 checkpoint-3640 Complete\n",
      "output_dir_1e-06_fold_1 checkpoint-728 Complete\n",
      "output_dir_1e-06_fold_1 Successfully Saved\n",
      "output_dir_1e-06_fold_2 checkpoint-1456 Complete\n",
      "output_dir_1e-06_fold_2 checkpoint-2184 Complete\n",
      "output_dir_1e-06_fold_2 checkpoint-2912 Complete\n",
      "output_dir_1e-06_fold_2 checkpoint-3640 Complete\n",
      "output_dir_1e-06_fold_2 checkpoint-728 Complete\n",
      "output_dir_1e-06_fold_2 Successfully Saved\n",
      "output_dir_1e-06_fold_3 checkpoint-1456 Complete\n",
      "output_dir_1e-06_fold_3 checkpoint-2184 Complete\n",
      "output_dir_1e-06_fold_3 checkpoint-2912 Complete\n",
      "output_dir_1e-06_fold_3 checkpoint-3640 Complete\n",
      "output_dir_1e-06_fold_3 checkpoint-728 Complete\n",
      "output_dir_1e-06_fold_3 Successfully Saved\n",
      "output_dir_1e-06_fold_4 checkpoint-1456 Complete\n",
      "output_dir_1e-06_fold_4 checkpoint-2184 Complete\n",
      "output_dir_1e-06_fold_4 checkpoint-2912 Complete\n",
      "output_dir_1e-06_fold_4 checkpoint-3640 Complete\n",
      "output_dir_1e-06_fold_4 checkpoint-728 Complete\n",
      "output_dir_1e-06_fold_4 Successfully Saved\n",
      "output_dir_1e-06_fold_5 checkpoint-1456 Complete\n",
      "output_dir_1e-06_fold_5 checkpoint-2184 Complete\n",
      "output_dir_1e-06_fold_5 checkpoint-2912 Complete\n",
      "output_dir_1e-06_fold_5 checkpoint-3640 Complete\n",
      "output_dir_1e-06_fold_5 checkpoint-728 Complete\n",
      "output_dir_1e-06_fold_5 Successfully Saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Checkpoint</th>\n",
       "      <th>Weight_Decay</th>\n",
       "      <th>Fold</th>\n",
       "      <th>EVENT_F1</th>\n",
       "      <th>CAUSE_F1</th>\n",
       "      <th>ACTION_F1</th>\n",
       "      <th>Micro_F1</th>\n",
       "      <th>Macro_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1516</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2274</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3032</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3790</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>758</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1456</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>5</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2184</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>5</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2912</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>5</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3640</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>5</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>728</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>5</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Checkpoint  Weight_Decay  Fold EVENT_F1 CAUSE_F1 ACTION_F1 Micro_F1  \\\n",
       "0         1516      0.000100     1    0.917    0.816     0.806    0.854   \n",
       "0         2274      0.000100     1    0.919    0.816       0.8    0.852   \n",
       "0         3032      0.000100     1    0.914    0.815     0.802     0.85   \n",
       "0         3790      0.000100     1    0.914    0.812     0.794    0.847   \n",
       "0          758      0.000100     1     0.91    0.776     0.794    0.837   \n",
       "..         ...           ...   ...      ...      ...       ...      ...   \n",
       "0         1456      0.000001     5    0.961    0.923     0.928     0.94   \n",
       "0         2184      0.000001     5    0.968    0.935     0.936    0.948   \n",
       "0         2912      0.000001     5    0.972     0.94     0.931     0.95   \n",
       "0         3640      0.000001     5    0.972    0.942     0.941    0.953   \n",
       "0          728      0.000001     5     0.96     0.91     0.912    0.931   \n",
       "\n",
       "   Macro_F1  \n",
       "0     0.846  \n",
       "0     0.845  \n",
       "0     0.844  \n",
       "0      0.84  \n",
       "0     0.826  \n",
       "..      ...  \n",
       "0     0.937  \n",
       "0     0.946  \n",
       "0     0.948  \n",
       "0     0.951  \n",
       "0     0.927  \n",
       "\n",
       "[125 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code will take a LONG time depending on how many models you have. It is reccommended to use a GPU\n",
    "path = \"HRAF_Model_MultiLabel_ThreeLargeClasses_kfoldsDEMO_WeightInvestigation\"\n",
    "\n",
    "df_allScores = checkpointInfer(path=path, data=Hraf, labels=labels, tokenizer_kwargs=tokenizer_kwargs,  classifier_kwargs=classifier_kwargs, folds=True, output_str=\"output_dir_\", modelDestinctifier= \"Weight_Decay\")\n",
    "df_allScores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional File save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HrafOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468d823866d34e7f80f1e3dc120f38b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/728 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# optionally save the file to json\n",
    "from transformers import AutoTokenizer\n",
    "import copy\n",
    "\n",
    "HrafOutput_dummy = copy.deepcopy(HrafOutput)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"passage\"], truncation=True)\n",
    "\n",
    "tokenized_Hraf = Hraf.map(preprocess_function, batched=True)\n",
    "\n",
    "for index, passage in enumerate(HrafOutput_dummy):\n",
    "    assert passage['passage'] == tokenized_Hraf[index]['passage']\n",
    "    passage['pred_labels'] = {key:passage['pred_labels'][index] for index, key in enumerate(labels)}\n",
    "    passage['actual_labels'] = {key:passage['actual_labels'][index] for index, key in enumerate(labels)}\n",
    "    passage['input_ids'] = tokenized_Hraf[index]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Save to unformatted json (uncomment)\n",
    "with open(f\"Datasets/tokenized_inputs.json\", \"w\") as outfile:\n",
    "    json.dump(HrafOutput_dummy, outfile)\n",
    "\n",
    "\n",
    "# # Save to Dataset (uncomment)\n",
    "# HrafOutput_dummy_dataset = Dataset.from_list(HrafOutput_dummy)\n",
    "# Dataset.to_json(HrafOutput_dummy_dataset, f\"Datasets/tokenized_Hraf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHi Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1167,  351],\n",
       "       [  49,  183]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "ct_EVENT_CAUSE = pd.crosstab(df[('EVENT','No_Info')], df[('CAUSE','No_Info')], rownames=['ACTION'], colnames=['CAUSE'])\n",
    "ct_EVENT_CAUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT by CAUSE:\n",
      "chi: 292.4\n",
      "p:   0.0\n",
      "\n",
      "\n",
      "EVENT by ACTION:\n",
      "chi: 103.3\n",
      "p:   0.0\n",
      "\n",
      "\n",
      "ACTION by CAUSE:\n",
      "chi: 0.0\n",
      "p:   0.857\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chi_square_calc(row, col):\n",
    "    cross_tab = pd.crosstab(df[(row,'No_Info')], df[(col,'No_Info')], rownames=[row], colnames=[col])\n",
    "    stat, p, dof, expected = chi2_contingency(cross_tab)\n",
    "    results = f\"{row} by {col}:\\nchi: {round(stat,1)}\\np:   {round(p,3)}\\n\\n\"\n",
    "    return results\n",
    "\n",
    "group_list = [('EVENT', 'CAUSE'), ('EVENT', 'ACTION'), ('ACTION', 'CAUSE')]\n",
    "for row, col in group_list:\n",
    "    print(chi_square_calc(row, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_sqr(obs):\n",
    "    size_x = obs.shape\n",
    "    chi_mat = np.zeros(size_x)\n",
    "    for row in range(size_x[0]):\n",
    "        for col in range(size_x[1]):\n",
    "            exp = np.sum(x[row]) * np.sum(x[:,col]) / np.sum(x)\n",
    "            chi_mat[row, col] = np.sum((obs[row, col] - exp)**2 / exp)\n",
    "    return chi_mat\n",
    "\n",
    "print(np.sum(chi_sqr(x)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
