{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "# import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "# from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# # If below code does not work, copy and paste this code in the terminal: huggingface-cli login \n",
    "# then paste your token\n",
    "\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of passages and predict them (will take about .25 seconds per passage for me so beware the wait)\n",
    "def predictor(data, labels, tokenizer_kwargs, classifier):\n",
    "    dataOutput = []\n",
    "    for text in data:\n",
    "        # get actual labels\n",
    "        actual_labels = [text[label] for label in labels]\n",
    "\n",
    "\n",
    "        # get predicted labels\n",
    "        prediction = classifier(text['passage'], **tokenizer_kwargs)\n",
    "        scores = {item['label']:item['score'] for item in prediction[0]} #turn prediction into a dictionary\n",
    "        pred_labels = [1 if scores[label] >= 0.5 else 0 for label in labels]\n",
    "\n",
    "        \n",
    "        output_dict = dict()\n",
    "        output_dict[\"pred_labels\"] = pred_labels\n",
    "        output_dict[\"actual_labels\"] = actual_labels\n",
    "        output_dict[\"passage\"] = text['passage']\n",
    "        output_dict[\"ID\"] = text['ID']\n",
    "\n",
    "\n",
    "        # score[0][(\"actual_label\", 'passage')] = text['passage'], text['label']\n",
    "        dataOutput.append(output_dict)\n",
    "    return dataOutput\n",
    "\n",
    "# Get F1 scores\n",
    "def score(dataOutput, labels):\n",
    "    from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "    df_score = pd.DataFrame(index=['NLP'], columns= [label+\"_F1\" for label in labels] + [\"Micro_F1\", \"Macro_F1\"])\n",
    "    actual_labels = [x['actual_labels'] for x in dataOutput]\n",
    "    pred_labels = [x['pred_labels'] for x in dataOutput]\n",
    "    for index, label in enumerate(labels):\n",
    "        f1 = round(f1_score(y_true=np.array(actual_labels)[:,index], y_pred=np.array(pred_labels)[:,index]),3)\n",
    "        df_score.at['NLP', label+\"_F1\"] = f1\n",
    "        # print(f\"{label}: {(6 - len(label)) *' '}{f1}\")\n",
    "\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    f1_micro = round(f1_score(y_true=actual_labels, y_pred=pred_labels, average='micro'),3)\n",
    "    f1_macro = round(f1_score(y_true=actual_labels, y_pred=pred_labels, average='macro'),3)\n",
    "    df_score.at['NLP', \"Micro_F1\"] = f1_micro\n",
    "    df_score.at['NLP', \"Macro_F1\"] = f1_macro\n",
    "    return df_score\n",
    "def cor_score(dataOutput, labels):\n",
    "    from sklearn.metrics import matthews_corrcoef\n",
    "    df_score = pd.DataFrame(index=['NLP'], columns= [label+\"_Cor\" for label in labels])\n",
    "    actual_labels = [x['actual_labels'] for x in dataOutput]\n",
    "    pred_labels = [x['pred_labels'] for x in dataOutput]\n",
    "    for index, label in enumerate(labels):\n",
    "        corrcoef = round(matthews_corrcoef(y_true=np.array(actual_labels)[:,index], y_pred=np.array(pred_labels)[:,index]),3)\n",
    "        df_score.at['NLP', label+\"_Cor\"] = corrcoef    \n",
    "    return df_score\n",
    "    # print(f'F1 score (micro) {f1_micro}\\nF1 score (macro) {f1_macro}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'passage', 'EVENT', 'EVENT_Illness', 'EVENT_Accident', 'EVENT_Other', 'CAUSE', 'CAUSE_Just_Happens', 'CAUSE_Material_Physical', 'CAUSE_Spirits_Gods', 'CAUSE_Witchcraft_Sorcery', 'CAUSE_Rule_Violation_Taboo', 'CAUSE_Other', 'ACTION', 'ACTION_Physical_Material', 'ACTION_Technical_Specialist', 'ACTION_Divination', 'ACTION_Shaman_Medium_Healer', 'ACTION_Priest_High_Religion', 'ACTION_Other'],\n",
       "    num_rows: 2074\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "loc = \"\"\n",
    "# loc = \"../HRAF_MultiLabel_ThreeLargeClasses/\" #load old threemain class (comment this out unless you specifically are using it)\n",
    "\n",
    "# dataset = load_dataset('csv', data_files={'train': 'train.txt', 'validation': 'val.txt', 'test': 'test.txt'}, sep=\";\", \n",
    "#                               names=[\"text\", \"label\"])\n",
    "\n",
    "\n",
    "f = open(loc+\"Datasets/test_dataset.json\")\n",
    "# f = open(\"../HRAF_MultiLabel_ThreeLargeClasses/Datasets/test_dataset.json\") #load old threemain class (comment this out unless you specifically are using it)\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "Hraf = Dataset.from_dict(data)\n",
    "Hraf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Kwargs and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EVENT',\n",
       " 'EVENT_Illness',\n",
       " 'EVENT_Accident',\n",
       " 'EVENT_Other',\n",
       " 'CAUSE',\n",
       " 'CAUSE_Just_Happens',\n",
       " 'CAUSE_Material_Physical',\n",
       " 'CAUSE_Spirits_Gods',\n",
       " 'CAUSE_Witchcraft_Sorcery',\n",
       " 'CAUSE_Rule_Violation_Taboo',\n",
       " 'CAUSE_Other',\n",
       " 'ACTION',\n",
       " 'ACTION_Physical_Material',\n",
       " 'ACTION_Technical_Specialist',\n",
       " 'ACTION_Divination',\n",
       " 'ACTION_Shaman_Medium_Healer',\n",
       " 'ACTION_Priest_High_Religion',\n",
       " 'ACTION_Other']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define tokenizer kwargs\n",
    "tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}\n",
    "\n",
    "classifier_kwargs = {'top_k':None, 'device':0} #Set device -1 for CPU, 0 or higher for GPU\n",
    "\n",
    "# get label names\n",
    "labels = [label for label in Hraf.features.keys() if label not in ['ID', 'passage']]\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this or the other model, not both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initial single run model, mostly derived from the simplistic multi-label classification task. You may delete but be cautious\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import TrainingArguments, Trainer, DistilBertForSequenceClassification, PreTrainedModel, PretrainedConfig, DataCollatorWithPadding, DistilBertModel, AutoConfig\n",
    "\n",
    "\n",
    "class SubLabelClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SubLabelClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)    ## Linear is a simple linear transformation layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "## One big mistake with NNs is making sure the input/output layers’ content/dimensions match up; \n",
    "# note the above has 2 Linear layers like this: input -> hidden; hidden -> output\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create sample model\n",
    "class HierarchicalMultiLabelClassifier(PreTrainedModel):\n",
    "    def __init__(self, model_name, num_main_labels, num_event_labels, num_cause_labels, num_action_labels, config):\n",
    "        super(HierarchicalMultiLabelClassifier, self).__init__(config)\n",
    "        # # Not sure the purpose of these here, maybe a fluke with GPT or is for later reference donw the line, probably not needed here\n",
    "        # self.num_main_labels = num_main_labels\n",
    "        # self.num_event_labels = num_event_labels\n",
    "        # self.num_cause_labels = num_cause_labels\n",
    "        # self.num_action_labels = num_action_labels\n",
    "        \n",
    "        # self.distilbert = DistilBertForSequenceClassification.from_pretrained(model_name) \n",
    "        self.distilbert = DistilBertModel.from_pretrained(model_name, config=config) # This does not use the head for sequence classification, I am wondering if that is a disadvantage\n",
    "        hidden_size = self.distilbert.config.hidden_size\n",
    "        \n",
    "        # logits for each classifier group (might only need main and sublabel groups...)\n",
    "        self.main_classifier = nn.Linear(hidden_size, num_main_labels)\n",
    "        self.event_classifier = SubLabelClassifier(input_dim = self.distilbert.config.hidden_size, hidden_dim = 50, output_dim = num_event_labels)\n",
    "        self.cause_classifier = SubLabelClassifier(input_dim = self.distilbert.config.hidden_size, hidden_dim = 50, output_dim = num_cause_labels)\n",
    "        self.action_classifier = SubLabelClassifier(input_dim = self.distilbert.config.hidden_size, hidden_dim = 50, output_dim = num_action_labels)\n",
    "        #\n",
    "    def forward(self, input_ids, attention_mask): #(should the argument params be \"tensor\" instead like the huggingface example?)\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs[0]  # Last hidden state\n",
    "        pooled_output = hidden_state[:, 0]  # [CLS] token\n",
    "        \n",
    "        main_logits = self.main_classifier(pooled_output)\n",
    "        event_logits = self.event_classifier(pooled_output) #previously outputs.last_hidden_state as per chat gpt. It was changed also because of chat GPT but also because the sizes of the logits and the labels did not match \n",
    "        cause_logits = self.cause_classifier(pooled_output)\n",
    "        action_logits = self.action_classifier(pooled_output)\n",
    "\n",
    "        # #Delete and uncomment above, this is just to circumvent the head sequence classification issues\n",
    "        # main_logits = None\n",
    "        # event_logits = None\n",
    "        # cause_logits = None\n",
    "        # action_logits = None\n",
    "        \n",
    "        return main_logits, event_logits, cause_logits, action_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attempt to create a inference model which uses a hierarchical structure of classification. \n",
    "## The output structure from this looks somewhat good as far as I can tell, but the predictions are terrible leading me to believe it might not be working as intended\n",
    "## You may delete but it is best to keep until we get a more working inference model\n",
    "from transformers import Pipeline\n",
    "\n",
    "class CustomClassificationPipeline(Pipeline):\n",
    "    def __init__(self, model, tokenizer,  **kwargs):\n",
    "        super().__init__(model=model, tokenizer=tokenizer, **kwargs)\n",
    "    \n",
    "    def __call__(self, texts, threshold=True, **kwargs):\n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        # Apply sigmoid (since it's a multi-label classification problem)\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        main_logits, event_logits, cause_logits, action_logits = outputs\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        main_probs = sigmoid(main_logits).detach().cpu().numpy()\n",
    "        event_probs = sigmoid(event_logits).detach().cpu().numpy()\n",
    "        cause_probs = sigmoid(cause_logits).detach().cpu().numpy()\n",
    "        action_probs = sigmoid(action_logits).detach().cpu().numpy()\n",
    "\n",
    "        # Define label categories for each set of outputs\n",
    "        main_labels = [\"EVENT\", \"CAUSE\", \"ACTION\"]\n",
    "        event_labels = [\"EVENT_Illness\", \"EVENT_Accident\", \"EVENT_Other\"]\n",
    "        cause_labels = [\"CAUSE_Just_Happens\", \"CAUSE_Material_Physical\", \"CAUSE_Spirits_Gods\", \"CAUSE_Witchcraft_Sorcery\", \"CAUSE_Rule_Violation_Taboo\",\"CAUSE_Other\"]\n",
    "        action_labels = [\"ACTION_Physical_Material\", \"ACTION_Technical_Specialist\", \"ACTION_Divination\", \"ACTION_Shaman_Medium_Healer\",\"ACTION_Priest_High_Religion\",\"ACTION_Other\"]\n",
    "\n",
    "        # Generate the predictions for each category (if threshold is desired)\n",
    "        if threshold:\n",
    "            main_predictions = [(main_labels[i], prob) for i, prob in enumerate(main_probs[0]) if prob > 0.5]\n",
    "            event_predictions = [(event_labels[i], prob) for i, prob in enumerate(event_probs[0]) if prob > 0.5]\n",
    "            cause_predictions = [(cause_labels[i], prob) for i, prob in enumerate(cause_probs[0]) if prob > 0.5]\n",
    "            action_predictions = [(action_labels[i], prob) for i, prob in enumerate(action_probs[0]) if prob > 0.5]\n",
    "        elif threshold==False:\n",
    "            main_predictions = [(main_labels[i], prob) for i, prob in enumerate(main_probs[0])]\n",
    "            event_predictions = [(event_labels[i], prob) for i, prob in enumerate(event_probs[0])]\n",
    "            cause_predictions = [(cause_labels[i], prob) for i, prob in enumerate(cause_probs[0])]\n",
    "            action_predictions = [(action_labels[i], prob) for i, prob in enumerate(action_probs[0])]\n",
    "\n",
    "        return {\n",
    "            \"main_predictions\": main_predictions,\n",
    "            \"event_predictions\": event_predictions,\n",
    "            \"cause_predictions\": cause_predictions,\n",
    "            \"action_predictions\": action_predictions\n",
    "        }\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Handle parameters passed to the pipeline and return cleaned or adjusted versions.\n",
    "        \"\"\"\n",
    "        preprocess_kwargs = {}\n",
    "        forward_kwargs = {}\n",
    "        postprocess_kwargs = {}\n",
    "        return preprocess_kwargs, forward_kwargs, postprocess_kwargs\n",
    "\n",
    "    def preprocess(self, inputs):\n",
    "        \"\"\"\n",
    "        Preprocess the inputs (tokenization, truncation, etc.).\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        return self.tokenizer(inputs, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \"\"\"\n",
    "        # Forward pass through the model\n",
    "        outputs = self.model(**model_inputs)\n",
    "        return outputs\n",
    "    def postprocess(self, model_outputs):\n",
    "        \"\"\"\n",
    "        Process the raw model outputs (logits) into human-readable format (e.g., probabilities, labels).\n",
    "        \"\"\"\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        # Assuming your model outputs a tuple of logits for different label categories\n",
    "        main_logits, event_logits, cause_logits, action_logits = model_outputs\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        main_probs = sigmoid(main_logits).detach().cpu().numpy()\n",
    "        event_probs = sigmoid(event_logits).detach().cpu().numpy()\n",
    "        cause_probs = sigmoid(cause_logits).detach().cpu().numpy()\n",
    "        action_probs = sigmoid(action_logits).detach().cpu().numpy()\n",
    "\n",
    "        # Define label categories for each set of outputs\n",
    "        main_labels = [\"EVENT\", \"CAUSE\", \"ACTION\"]\n",
    "        event_labels = [\"EVENT_Illness\", \"EVENT_Accident\", \"EVENT_Other\"]\n",
    "        cause_labels = [\"CAUSE_Just_Happens\", \"CAUSE_Material_Physical\", \"CAUSE_Spirits_Gods\", \"CAUSE_Witchcraft_Sorcery\", \"CAUSE_Rule_Violation_Taboo\",\"CAUSE_Other\"]\n",
    "        action_labels = [\"ACTION_Physical_Material\", \"ACTION_Technical_Specialist\", \"ACTION_Divination\", \"ACTION_Shaman_Medium_Healer\",\"ACTION_Priest_High_Religion\",\"ACTION_Other\"]\n",
    "\n",
    "        # Generate the predictions for each category\n",
    "        main_predictions = [(main_labels[i], prob) for i, prob in enumerate(main_probs[0]) if prob > 0.5]\n",
    "        event_predictions = [(event_labels[i], prob) for i, prob in enumerate(event_probs[0]) if prob > 0.5]\n",
    "        cause_predictions = [(cause_labels[i], prob) for i, prob in enumerate(cause_probs[0]) if prob > 0.5]\n",
    "        action_predictions = [(action_labels[i], prob) for i, prob in enumerate(action_probs[0]) if prob > 0.5]\n",
    "\n",
    "        # Return the predictions in a structured format\n",
    "        return {\n",
    "            \"main_predictions\": main_predictions,\n",
    "            \"event_predictions\": event_predictions,\n",
    "            \"cause_predictions\": cause_predictions,\n",
    "            \"action_predictions\": action_predictions\n",
    "        }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DUMMY DELETE (try using custom pretrained model)\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "model_path = \"Model_1_BaseTest/Hierarchy_test_fold_1\"\n",
    "checkpoint_path = \"checkpoint-10790\"\n",
    "import os\n",
    "path =os.path.abspath(f\"{model_path}/{checkpoint_path}\")\n",
    "model_name = path\n",
    "num_main_labels = 3 # For EVENT, CAUSE, ACTION\n",
    "num_event_labels = 3\n",
    "num_cause_labels = 6\n",
    "num_action_labels = 6\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = HierarchicalMultiLabelClassifier(model_name, num_main_labels, num_event_labels, num_cause_labels, num_action_labels, config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "# model = HierarchicalMultiLabelClassifier.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'main_predictions': [('EVENT', 0.6270628), ('ACTION', 0.54589695)],\n",
       " 'event_predictions': [('EVENT_Other', 0.63102037)],\n",
       " 'cause_predictions': [('CAUSE_Just_Happens', 0.5368507),\n",
       "  ('CAUSE_Material_Physical', 0.537756)],\n",
       " 'action_predictions': [('ACTION_Physical_Material', 0.5223775),\n",
       "  ('ACTION_Shaman_Medium_Healer', 0.5379956),\n",
       "  ('ACTION_Other', 0.50957406)]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Use hierarchical version of inference and display a demo of the predictions\n",
    "## dummy delete\n",
    "classifier = CustomClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "text = '''\n",
    "“Drinking-tubes made of the leg-bones of swans (Fig. 109) are 190 also used chiefly as a measure of precaution against diseases subject to shunning.....”\n",
    "'''\n",
    "tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}\n",
    "\n",
    "classifier_kwargs = {'top_k':None, 'device':0} #Set device -1 for CPU, 0 or higher for GPU\n",
    "prediction = classifier(text, **tokenizer_kwargs)\n",
    "prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /Users/ericchantland/Library/CloudStorage/Dropbox/MEM-DEV-LAB-Current/2023-eHRAF-Misf/HRAF-Misf-NaturalLanguageProcessing/HRAF_NLP/HRAF_MultiLabel_Hierarchical/Model_1_BaseTest/Hierarchy_test_fold_1/checkpoint-10790 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'ACTION_Technical_Specialist', 'score': 0.06796205788850784},\n",
       "  {'label': 'EVENT_Illness', 'score': 0.06729385256767273},\n",
       "  {'label': 'CAUSE_Spirits_Gods', 'score': 0.06675578653812408},\n",
       "  {'label': 'EVENT_Accident', 'score': 0.061082128435373306},\n",
       "  {'label': 'ACTION_Shaman_Medium_Healer', 'score': 0.05792394280433655},\n",
       "  {'label': 'CAUSE', 'score': 0.0577295646071434},\n",
       "  {'label': 'ACTION_Divination', 'score': 0.05683483928442001},\n",
       "  {'label': 'CAUSE_Just_Happens', 'score': 0.05545376241207123},\n",
       "  {'label': 'ACTION_Other', 'score': 0.05515432357788086},\n",
       "  {'label': 'CAUSE_Rule_Violation_Taboo', 'score': 0.05494187772274017},\n",
       "  {'label': 'CAUSE_Witchcraft_Sorcery', 'score': 0.05452914163470268},\n",
       "  {'label': 'EVENT_Other', 'score': 0.05282062664628029},\n",
       "  {'label': 'ACTION_Priest_High_Religion', 'score': 0.052195955067873},\n",
       "  {'label': 'CAUSE_Other', 'score': 0.05177311599254608},\n",
       "  {'label': 'CAUSE_Material_Physical', 'score': 0.05050887539982796},\n",
       "  {'label': 'ACTION', 'score': 0.04858081787824631},\n",
       "  {'label': 'ACTION_Physical_Material', 'score': 0.04799750819802284},\n",
       "  {'label': 'EVENT', 'score': 0.04046187922358513}]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt to use simplistic initial classification (note that the output is a single list which is likely not preferable.)\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# CHANGE Model name\n",
    "# model = \"Model_1_BaseTest/Hierarchy_test_fold_1\"\n",
    "# checkpoint_path = \"checkpoint-10790\"\n",
    "\n",
    "model = \"Model_1_BaseTest/Hierarchy_test_fold_1\"\n",
    "checkpoint_path = \"checkpoint-10790\"\n",
    "# set up the pipeline from local\n",
    "import os\n",
    "path =os.path.abspath(f\"{model}/{checkpoint_path}\")\n",
    "classifier = pipeline(\"text-classification\", model=path, **classifier_kwargs)\n",
    "\n",
    "\n",
    "# sample inference ENTER TEXT IN HERE.\n",
    "text = '''\n",
    "“Drinking-tubes made of the leg-bones of swans (Fig. 109) are 190 also used chiefly as a measure of precaution against diseases subject to shunning.....”\n",
    "'''\n",
    "# reveal sample classification\n",
    "prediction = classifier(text, **tokenizer_kwargs)\n",
    "prediction\n",
    "\n",
    "# # Demo other models (COMMENT THIS OUT UNLESS YOU REALLY WANT TO DEMO THIS)\n",
    "# # Set up path from online hub (note, this is analogous but different model and is here because this is a demo)\n",
    "# classifier = pipeline(\"text-classification\", top_k=None, model=\"Chantland/Hraf_MultiLabel\", use_auth_token=\"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\", tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\"))\n",
    "# model = \"MultiLabel_ThreeLargeClasses\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Predict dataset (may take about .25 seconds per passage when tested on lab mac, could differ depending on your system)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Also note that this pipeline is sequential and may give a warning saying it is unoptimized. Currently, using a whole dataset does not seem to reap faster results so we are remaining with sequential\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m HrafOutput \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHraf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(HrafOutput), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassages Predicted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m, in \u001b[0;36mpredictor\u001b[0;34m(data, labels, tokenizer_kwargs, classifier)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# get predicted labels\u001b[39;00m\n\u001b[1;32m     10\u001b[0m prediction \u001b[38;5;241m=\u001b[39m classifier(text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassage\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n\u001b[0;32m---> 11\u001b[0m scores \u001b[38;5;241m=\u001b[39m {item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]:item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mprediction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m} \u001b[38;5;66;03m#turn prediction into a dictionary\u001b[39;00m\n\u001b[1;32m     12\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scores[label] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[1;32m     15\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Predict dataset (may take about .25 seconds per passage when tested on lab mac, could differ depending on your system)\n",
    "# Also note that this pipeline is sequential and may give a warning saying it is unoptimized. Currently, using a whole dataset does not seem to reap faster results so we are remaining with sequential\n",
    "HrafOutput = predictor(Hraf, labels=labels, tokenizer_kwargs=tokenizer_kwargs, classifier=classifier)\n",
    "print(len(HrafOutput), \"passages Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HrafOutput = classifier(Hraf['passage'],**tokenizer_kwargs)\n",
    "# print(len(HrafOutput), \"passages Predicted\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate \"Correctness\" Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EVENT_F1</th>\n",
       "      <th>EVENT_Illness_F1</th>\n",
       "      <th>EVENT_Accident_F1</th>\n",
       "      <th>EVENT_Other_F1</th>\n",
       "      <th>CAUSE_F1</th>\n",
       "      <th>CAUSE_Just_Happens_F1</th>\n",
       "      <th>CAUSE_Material_Physical_F1</th>\n",
       "      <th>CAUSE_Spirits_Gods_F1</th>\n",
       "      <th>CAUSE_Witchcraft_Sorcery_F1</th>\n",
       "      <th>CAUSE_Rule_Violation_Taboo_F1</th>\n",
       "      <th>CAUSE_Other_F1</th>\n",
       "      <th>ACTION_F1</th>\n",
       "      <th>ACTION_Physical_Material_F1</th>\n",
       "      <th>ACTION_Technical_Specialist_F1</th>\n",
       "      <th>ACTION_Divination_F1</th>\n",
       "      <th>ACTION_Shaman_Medium_Healer_F1</th>\n",
       "      <th>ACTION_Priest_High_Religion_F1</th>\n",
       "      <th>ACTION_Other_F1</th>\n",
       "      <th>Micro_F1</th>\n",
       "      <th>Macro_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    EVENT_F1 EVENT_Illness_F1 EVENT_Accident_F1 EVENT_Other_F1 CAUSE_F1  \\\n",
       "NLP      0.0              0.0               0.0            0.0      0.0   \n",
       "\n",
       "    CAUSE_Just_Happens_F1 CAUSE_Material_Physical_F1 CAUSE_Spirits_Gods_F1  \\\n",
       "NLP                   0.0                        0.0                   0.0   \n",
       "\n",
       "    CAUSE_Witchcraft_Sorcery_F1 CAUSE_Rule_Violation_Taboo_F1 CAUSE_Other_F1  \\\n",
       "NLP                         0.0                           0.0            0.0   \n",
       "\n",
       "    ACTION_F1 ACTION_Physical_Material_F1 ACTION_Technical_Specialist_F1  \\\n",
       "NLP       0.0                         0.0                            0.0   \n",
       "\n",
       "    ACTION_Divination_F1 ACTION_Shaman_Medium_Healer_F1  \\\n",
       "NLP                  0.0                            0.0   \n",
       "\n",
       "    ACTION_Priest_High_Religion_F1 ACTION_Other_F1 Micro_F1 Macro_F1  \n",
       "NLP                            0.0             0.0      0.0      0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get F1 scores for labels.\n",
    "df_score = score(HrafOutput, labels)\n",
    "# Optional TEST, get correlational score instead)\n",
    "df_score = score(HrafOutput, labels)\n",
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EVENT_Cor</th>\n",
       "      <th>EVENT_Illness_Cor</th>\n",
       "      <th>EVENT_Accident_Cor</th>\n",
       "      <th>EVENT_Other_Cor</th>\n",
       "      <th>CAUSE_Cor</th>\n",
       "      <th>CAUSE_Just_Happens_Cor</th>\n",
       "      <th>CAUSE_Material_Physical_Cor</th>\n",
       "      <th>CAUSE_Spirits_Gods_Cor</th>\n",
       "      <th>CAUSE_Witchcraft_Sorcery_Cor</th>\n",
       "      <th>CAUSE_Rule_Violation_Taboo_Cor</th>\n",
       "      <th>CAUSE_Other_Cor</th>\n",
       "      <th>ACTION_Cor</th>\n",
       "      <th>ACTION_Physical_Material_Cor</th>\n",
       "      <th>ACTION_Technical_Specialist_Cor</th>\n",
       "      <th>ACTION_Divination_Cor</th>\n",
       "      <th>ACTION_Shaman_Medium_Healer_Cor</th>\n",
       "      <th>ACTION_Priest_High_Religion_Cor</th>\n",
       "      <th>ACTION_Other_Cor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    EVENT_Cor EVENT_Illness_Cor EVENT_Accident_Cor EVENT_Other_Cor CAUSE_Cor  \\\n",
       "NLP       0.0               0.0                0.0             0.0       0.0   \n",
       "\n",
       "    CAUSE_Just_Happens_Cor CAUSE_Material_Physical_Cor CAUSE_Spirits_Gods_Cor  \\\n",
       "NLP                    0.0                         0.0                    0.0   \n",
       "\n",
       "    CAUSE_Witchcraft_Sorcery_Cor CAUSE_Rule_Violation_Taboo_Cor  \\\n",
       "NLP                          0.0                            0.0   \n",
       "\n",
       "    CAUSE_Other_Cor ACTION_Cor ACTION_Physical_Material_Cor  \\\n",
       "NLP             0.0        0.0                          0.0   \n",
       "\n",
       "    ACTION_Technical_Specialist_Cor ACTION_Divination_Cor  \\\n",
       "NLP                             0.0                   0.0   \n",
       "\n",
       "    ACTION_Shaman_Medium_Healer_Cor ACTION_Priest_High_Religion_Cor  \\\n",
       "NLP                             0.0                             0.0   \n",
       "\n",
       "    ACTION_Other_Cor  \n",
       "NLP              0.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional TEST, get correlational score instead)\n",
    "df_score_cor = cor_score(HrafOutput, labels)\n",
    "df_score_cor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add correctness to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>EVENT_Illness_F1</th>\n",
       "      <th>EVENT_Accident_F1</th>\n",
       "      <th>EVENT_Other_F1</th>\n",
       "      <th>CAUSE_Material_Physical_F1</th>\n",
       "      <th>CAUSE_Spirits_Gods_F1</th>\n",
       "      <th>CAUSE_Witchcraft_Sorcery_F1</th>\n",
       "      <th>CAUSE_Rule_Violation_Taboo_F1</th>\n",
       "      <th>ACTION_Physical_Material_F1</th>\n",
       "      <th>ACTION_Technical_Specialist_F1</th>\n",
       "      <th>ACTION_Divination_F1</th>\n",
       "      <th>ACTION_Shaman_Medium_Healer_F1</th>\n",
       "      <th>ACTION_Priest_High_Religion_F1</th>\n",
       "      <th>Micro_F1</th>\n",
       "      <th>Macro_F1</th>\n",
       "      <th>test_length</th>\n",
       "      <th>train_length</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.47</td>\n",
       "      <td>2074</td>\n",
       "      <td>8293</td>\n",
       "      <td>model: Model_3_LearningRates/Learning_Rate_1e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2074</td>\n",
       "      <td>8293</td>\n",
       "      <td>model: Model_2_ReducedCols/Weight_Decay_.01_fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date EVENT_Illness_F1 EVENT_Accident_F1 EVENT_Other_F1  \\\n",
       "NLP 2024-05-29            0.856             0.355          0.574   \n",
       "NLP 2024-05-29            0.856                 0          0.473   \n",
       "\n",
       "    CAUSE_Material_Physical_F1 CAUSE_Spirits_Gods_F1  \\\n",
       "NLP                      0.394                 0.681   \n",
       "NLP                      0.065                 0.446   \n",
       "\n",
       "    CAUSE_Witchcraft_Sorcery_F1 CAUSE_Rule_Violation_Taboo_F1  \\\n",
       "NLP                        0.59                         0.493   \n",
       "NLP                           0                             0   \n",
       "\n",
       "    ACTION_Physical_Material_F1 ACTION_Technical_Specialist_F1  \\\n",
       "NLP                       0.628                          0.396   \n",
       "NLP                       0.559                              0   \n",
       "\n",
       "    ACTION_Divination_F1 ACTION_Shaman_Medium_Healer_F1  \\\n",
       "NLP                  0.0                          0.479   \n",
       "NLP                    0                              0   \n",
       "\n",
       "    ACTION_Priest_High_Religion_F1 Micro_F1 Macro_F1  test_length  \\\n",
       "NLP                          0.193    0.622     0.47         2074   \n",
       "NLP                              0    0.503      0.2         2074   \n",
       "\n",
       "     train_length                                              Notes  \n",
       "NLP          8293  model: Model_3_LearningRates/Learning_Rate_1e-...  \n",
       "NLP          8293  model: Model_2_ReducedCols/Weight_Decay_.01_fo...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# export F1 scores to excel\n",
    "df_scoresSep = df_score.copy()\n",
    "# first load train (and maybe add validation)\n",
    "f = open(loc+\"Datasets/train_dataset.json\")\n",
    "data = json.load(f)\n",
    "train = Dataset.from_dict(data)\n",
    "if os.path.isfile(loc+\"Datasets/validation_dataset.json\"):\n",
    "    f = open(loc+\"Datasets/validation_dataset.json\")\n",
    "    data = json.load(f)\n",
    "    valid = Dataset.from_dict(data)\n",
    "    train = concatenate_datasets([train, valid])\n",
    "# add lengths of test and training set\n",
    "df_scoresSep[[\"test_length\", \"train_length\"]] = (len(Hraf), len(train))\n",
    "# add date\n",
    "df_scoresSep.insert(0, \"Date\", [datetime.today().date()])\n",
    "if loc == \"\":\n",
    "    df_scoresSep['Notes'] = f\"model: {model}/{checkpoint_path}, Dataset: {model}\"\n",
    "else:\n",
    "    df_scoresSep['Notes'] = f\"model: {model}/{checkpoint_path}, Dataset: {loc}\"\n",
    "# load model_performance.xlsx or else create it\n",
    "if os.path.isfile(\"Model_Prediction_Performance.xlsx\"):\n",
    "    df_oldScores = pd.read_excel(\"Model_Prediction_Performance.xlsx\", index_col=0)\n",
    "    df_oldScores_merged = pd.concat([df_scoresSep, df_oldScores])\n",
    "    nonDateCols = df_oldScores_merged.columns[df_scoresSep.columns != 'Date']\n",
    "    if any(df_oldScores_merged.duplicated(subset=nonDateCols)): # don't append the data unless it is new\n",
    "        print(\"Duplicated scores found, skipping new addition\")\n",
    "        df_scoresSep = df_oldScores.copy()\n",
    "    else:\n",
    "        df_scoresSep = df_oldScores_merged.copy()\n",
    "        df_scoresSep['Date'] = df_scoresSep['Date'].astype('datetime64[ns]')\n",
    "        df_scoresSep.to_excel(\"Model_Prediction_Performance.xlsx\")\n",
    "else:\n",
    "    df_scoresSep['Date'] = df_scoresSep['Date'].astype('datetime64[ns]')\n",
    "    df_scoresSep.to_excel(f\"Model_Prediction_Performance.xlsx\")\n",
    "df_scoresSep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Multi-model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to run over MANY models and checkpoints to test and see which is the strongest. This is ran instead of the single model one above and should NOT be ran together with the single model (simply because they do different things)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for running through all checkpoints\n",
    "# code for running through all checkpoints\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "def checkpointInfer(path, data, labels, tokenizer_kwargs, classifier_kwargs, folds=True, output_str=\"output_dir_\", modelDestinctifier:str= \"ModelDistinctifierUnknown\"):\n",
    "    # Initiate Dataframe overall\n",
    "    df = pd.DataFrame([])\n",
    "\n",
    "    # Get all viable models\n",
    "    # Makes sure the model starts with the output string and is a directory\n",
    "    models = [name for name in os.listdir(path) if (name.startswith(output_str) and os.path.isdir(f\"{path}/{name}\"))]\n",
    "\n",
    "    for model in models:\n",
    "        # Initiate Dataframe for each model\n",
    "        df_model = pd.DataFrame([])\n",
    "\n",
    "        # Get checkpoint directory for a particular model and get the unit in which the model is distinguished (like learning rates)\n",
    "        checkpoints_dir = [checkpoint for checkpoint in os.listdir(f\"{path}/{model}\") if checkpoint.startswith(\"checkpoint\")]\n",
    "        modelDestinctifier_unit = re.findall(f\"{output_str}(.*?)_\",model)\n",
    "        try:\n",
    "            modelDestinctifier_unit = float(modelDestinctifier_unit[0])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        # Predict for each checkpoint (within said model) and save results\n",
    "        for checkpoint in checkpoints_dir:\n",
    "            # Initiate Dataframe for each checkpoint\n",
    "            df_checkpoint = pd.DataFrame([])\n",
    "            # set up the pipeline from local\n",
    "            model_path =os.path.abspath(f\"{path}/{model}/{checkpoint}\")\n",
    "            classifier = pipeline(\"text-classification\", model=model_path, **classifier_kwargs)\n",
    "            # Get Predictions\n",
    "            dataOutput = predictor(data, labels=labels, tokenizer_kwargs=tokenizer_kwargs, classifier=classifier)\n",
    "            # Get scores\n",
    "            df_checkpoint = score(dataOutput, labels)\n",
    "            df_checkpoint = df_checkpoint.reset_index(drop=True) #remove the index here\n",
    "\n",
    "\n",
    "            df_checkpoint.insert(0,modelDestinctifier,modelDestinctifier_unit) #insert model distinctifier (like weight decay or learning rate)\n",
    "            #Extract and add Fold name if relevant\n",
    "            if folds: #if using folds\n",
    "                fold = re.findall(r\"fold_(\\d*)\",model)\n",
    "                fold = int(fold[0])\n",
    "                df_checkpoint.insert(1,\"Fold\",fold)\n",
    "            else:\n",
    "                fold = \"\"\n",
    "\n",
    "            # get checkpoint\n",
    "            checkpoint_num = re.findall(r\"checkpoint-(\\d*)\",checkpoint)\n",
    "            assert len(checkpoint_num) == 1, f\"More or less than one checkpoint numbers found: {len(checkpoint_num)} checkpoints\"\n",
    "            checkpoint_num = int(checkpoint_num[0])\n",
    "\n",
    "            df_checkpoint.insert(0,\"Model\",model) # Add model name\n",
    "            df_checkpoint.insert(0,\"Checkpoint\",checkpoint_num)\n",
    "            df_model = pd.concat([df_model,df_checkpoint])\n",
    "            print(model, checkpoint, \"Complete\")\n",
    "\n",
    "        # concat model to overarching dataframe\n",
    "        df = pd.concat([df,df_model])\n",
    "        # save df for each model (as a checkpoint)\n",
    "        # import evaluation if it exists\n",
    "        if os.path.exists(f\"{path}/Inference_Test.xlsx\"):\n",
    "            old_df = pd.read_excel(f\"{path}/Inference_Test.xlsx\", sheet_name=\"Sheet1\", index_col=0)\n",
    "            df_model = pd.concat([old_df, df_model])\n",
    "\n",
    "        df_model.to_excel(f\"{path}/Inference_Test.xlsx\", sheet_name=\"Sheet1\")\n",
    "        print(model, \"Successfully Saved\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# output_str=\"output_dir_\"\n",
    "\n",
    "# model = \"MultiLabel_ThreeLargeClasses_kfoldsDEMO_WeightInvestigation\"\n",
    "# path =os.path.abspath(f\"HRAF_Model_{model}\")\n",
    "# x = [name for name in os.listdir(path) if (name.startswith(\"output_dir_\") and os.path.isdir(f\"{path}/{name}\"))]\n",
    "# # x\n",
    "# modelDestinctifier_unit = re.findall(f\"{output_str}(.*?)_\",x[1])\n",
    "# try:\n",
    "#     modelDestinctifier_unit = float(modelDestinctifier_unit)\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight_Decay_.01_fold_1 checkpoint-26430 Complete\n",
      "Weight_Decay_.01_fold_1 Successfully Saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Checkpoint</th>\n",
       "      <th>Weight_Decay</th>\n",
       "      <th>Fold</th>\n",
       "      <th>EVENT_Illness_F1</th>\n",
       "      <th>EVENT_Accident_F1</th>\n",
       "      <th>EVENT_Other_F1</th>\n",
       "      <th>CAUSE_Just_Happens_F1</th>\n",
       "      <th>CAUSE_Material_Physical_F1</th>\n",
       "      <th>CAUSE_Spirits_Gods_F1</th>\n",
       "      <th>CAUSE_Witchcraft_Sorcery_F1</th>\n",
       "      <th>CAUSE_Rule_Violation_Taboo_F1</th>\n",
       "      <th>CAUSE_Other_F1</th>\n",
       "      <th>ACTION_Physical_Material_F1</th>\n",
       "      <th>ACTION_Technical_Specialist_F1</th>\n",
       "      <th>ACTION_Divination_F1</th>\n",
       "      <th>ACTION_Shaman_Medium_Healer_F1</th>\n",
       "      <th>ACTION_Priest_High_Religion_F1</th>\n",
       "      <th>ACTION_Other_F1</th>\n",
       "      <th>Micro_F1</th>\n",
       "      <th>Macro_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26430</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Checkpoint  Weight_Decay  Fold EVENT_Illness_F1 EVENT_Accident_F1  \\\n",
       "0       26430          0.01     1            0.865               0.0   \n",
       "\n",
       "  EVENT_Other_F1 CAUSE_Just_Happens_F1 CAUSE_Material_Physical_F1  \\\n",
       "0          0.494                   0.0                      0.059   \n",
       "\n",
       "  CAUSE_Spirits_Gods_F1 CAUSE_Witchcraft_Sorcery_F1  \\\n",
       "0                 0.591                         0.0   \n",
       "\n",
       "  CAUSE_Rule_Violation_Taboo_F1 CAUSE_Other_F1 ACTION_Physical_Material_F1  \\\n",
       "0                           0.0            0.0                       0.567   \n",
       "\n",
       "  ACTION_Technical_Specialist_F1 ACTION_Divination_F1  \\\n",
       "0                            0.0                  0.0   \n",
       "\n",
       "  ACTION_Shaman_Medium_Healer_F1 ACTION_Priest_High_Religion_F1  \\\n",
       "0                          0.046                            0.0   \n",
       "\n",
       "  ACTION_Other_F1 Micro_F1 Macro_F1  \n",
       "0             0.0    0.501    0.175  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code will take a LONG time depending on how many models you have. It is recommended to use a GPU\n",
    "path = loc+\"/Model_3_LearningRates\"\n",
    "output_str = \"Learning_Rate_\"\n",
    "modelDestinctifier = \"Learning_Rate\"\n",
    "\n",
    "df_allScores = checkpointInfer(path=path, data=Hraf, labels=labels, tokenizer_kwargs=tokenizer_kwargs,  classifier_kwargs=classifier_kwargs, folds=True, output_str=output_str, modelDestinctifier= modelDestinctifier)\n",
    "df_allScores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional File save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HrafOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468d823866d34e7f80f1e3dc120f38b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/728 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# optionally save the file to json\n",
    "from transformers import AutoTokenizer\n",
    "import copy\n",
    "\n",
    "HrafOutput_dummy = copy.deepcopy(HrafOutput)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"passage\"], truncation=True)\n",
    "\n",
    "tokenized_Hraf = Hraf.map(preprocess_function, batched=True)\n",
    "\n",
    "for index, passage in enumerate(HrafOutput_dummy):\n",
    "    assert passage['passage'] == tokenized_Hraf[index]['passage']\n",
    "    passage['pred_labels'] = {key:passage['pred_labels'][index] for index, key in enumerate(labels)}\n",
    "    passage['actual_labels'] = {key:passage['actual_labels'][index] for index, key in enumerate(labels)}\n",
    "    passage['input_ids'] = tokenized_Hraf[index]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Save to unformatted json (uncomment)\n",
    "with open(f\"Datasets/tokenized_inputs.json\", \"w\") as outfile:\n",
    "    json.dump(HrafOutput_dummy, outfile)\n",
    "\n",
    "\n",
    "# # Save to Dataset (uncomment)\n",
    "# HrafOutput_dummy_dataset = Dataset.from_list(HrafOutput_dummy)\n",
    "# Dataset.to_json(HrafOutput_dummy_dataset, f\"Datasets/tokenized_Hraf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHi Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1167,  351],\n",
       "       [  49,  183]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "ct_EVENT_CAUSE = pd.crosstab(df[('EVENT','No_Info')], df[('CAUSE','No_Info')], rownames=['ACTION'], colnames=['CAUSE'])\n",
    "ct_EVENT_CAUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT by CAUSE:\n",
      "chi: 292.4\n",
      "p:   0.0\n",
      "\n",
      "\n",
      "EVENT by ACTION:\n",
      "chi: 103.3\n",
      "p:   0.0\n",
      "\n",
      "\n",
      "ACTION by CAUSE:\n",
      "chi: 0.0\n",
      "p:   0.857\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chi_square_calc(row, col):\n",
    "    cross_tab = pd.crosstab(df[(row,'No_Info')], df[(col,'No_Info')], rownames=[row], colnames=[col])\n",
    "    stat, p, dof, expected = chi2_contingency(cross_tab)\n",
    "    results = f\"{row} by {col}:\\nchi: {round(stat,1)}\\np:   {round(p,3)}\\n\\n\"\n",
    "    return results\n",
    "\n",
    "group_list = [('EVENT', 'CAUSE'), ('EVENT', 'ACTION'), ('ACTION', 'CAUSE')]\n",
    "for row, col in group_list:\n",
    "    print(chi_square_calc(row, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_sqr(obs):\n",
    "    size_x = obs.shape\n",
    "    chi_mat = np.zeros(size_x)\n",
    "    for row in range(size_x[0]):\n",
    "        for col in range(size_x[1]):\n",
    "            exp = np.sum(x[row]) * np.sum(x[:,col]) / np.sum(x)\n",
    "            chi_mat[row, col] = np.sum((obs[row, col] - exp)**2 / exp)\n",
    "    return chi_mat\n",
    "\n",
    "print(np.sum(chi_sqr(x)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
