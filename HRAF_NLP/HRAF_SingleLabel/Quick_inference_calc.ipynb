{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passage_uploader(data_label):\n",
    "    # load OLD and new datasets (THIS IS WHERE YOU WOULD ENTER IN THE DATA YOU WANTED TO TEST!)\n",
    "    df_old = pd.read_excel(\"../RA_Cleaning/Culture_Coding_old.xlsx\", header=[0,1], index_col=0)\n",
    "    df_current = pd.read_excel(\"../RA_Cleaning/Culture_Coding.xlsx\", header=[0,1], index_col=0)\n",
    "\n",
    "    # Remove the runs not of the first from the datasets (this will be made superfluous later but nonetheless is an extra assuredness step)\n",
    "    # df_old = df_old.loc[df_old[(\"CODER\",\"Run_Number\")]==1] #if this had \"Run_Number\" column, you would uncomment and run this line\n",
    "    df_current = df_current.loc[df_current[(\"CODER\",\"Run_Number\")]==1]\n",
    "\n",
    "    # only get new rows that have NOT been trained/tested on before\n",
    "    df_new = pd.concat([df_current, df_old])\n",
    "    df_new = df_new[~df_new.duplicated(subset=(\"CULTURE\",\"Passage Number\"), keep=False)]\n",
    "\n",
    "\n",
    "    # subdivide into just passage and outcome\n",
    "    df_small = pd.DataFrame()\n",
    "    df_small[[\"passage\",\"label\"]] = df_new[[('CULTURE', \"Passage\"), (data_label, \"No_Info\")]]\n",
    "    # Flip the lable of \"no_info\"\n",
    "    df_small[\"label\"] = df_small['label'].replace({0:1, 1:0})\n",
    "\n",
    "\n",
    "\n",
    "    # Create an NLP friendly dataset\n",
    "    Hraf = Dataset.from_dict(df_small.to_dict(orient= 'list'))\n",
    "    return Hraf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "\n",
    "def predictor(Hraf, classifier):\n",
    "    tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}\n",
    "    HrafOutput = []\n",
    "    for text in Hraf:\n",
    "        # text = Hraf[text]\n",
    "        score = classifier(text['passage'], **tokenizer_kwargs)\n",
    "        score[0][\"actual_label\"] = text['label']\n",
    "        score[0][\"passage\"] = text['passage']\n",
    "\n",
    "        # change the predicted label into a digit that we can compare\n",
    "        if score[0]['label'] == 'PRESENT':\n",
    "            score[0]['label'] = 1\n",
    "        elif score[0]['label'] == 'ABSENT':\n",
    "            score[0]['label'] = 0\n",
    "        else:\n",
    "            score[0]['label'] = 9\n",
    "\n",
    "\n",
    "        # score[0][(\"actual_label\", 'passage')] = text['passage'], text['label']\n",
    "        HrafOutput.append(score[0])\n",
    "\n",
    "    return HrafOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(HrafOutput):\n",
    "    # loading precision\n",
    "    precision = evaluate.load('precision')\n",
    "    result = precision.compute(predictions=[x['label'] for x in HrafOutput], references=[x['actual_label'] for x in HrafOutput])\n",
    "    # print(result)\n",
    "\n",
    "    # f1 score\n",
    "    from sklearn.metrics import f1_score, accuracy_score\n",
    "    accuracy = round(accuracy_score([x['actual_label'] for x in HrafOutput], [x['label'] for x in HrafOutput]),3)\n",
    "    f1 = round(f1_score([x['actual_label'] for x in HrafOutput], [x['label'] for x in HrafOutput], average = 'binary'),3)\n",
    "    f1_micro = round(f1_score([x['actual_label'] for x in HrafOutput], [x['label'] for x in HrafOutput], average='micro'),3)\n",
    "    f1_macro = round(f1_score([x['actual_label'] for x in HrafOutput], [x['label'] for x in HrafOutput], average='macro'),3)\n",
    "    results = f'Accuracy  {accuracy}\\nF1 score (base)  {f1}\\nF1 score (micro) {f1_micro}\\nF1 score (macro) {f1_macro}'\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HRAF_inference(training_label, data_label):\n",
    "    classifier = pipeline(\"text-classification\", model=f\"Chantland/HRAF_{training_label}_Demo\", use_auth_token=\"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\", tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\"))\n",
    "    Hraf = passage_uploader(data_label)\n",
    "    HrafOutput = predictor(Hraf, classifier)\n",
    "    results = f1_score(HrafOutput)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT data using EVENT model\n",
      " Accuracy  0.914\n",
      "F1 score (base)  0.935\n",
      "F1 score (micro) 0.914\n",
      "F1 score (macro) 0.905 \n",
      "\n",
      "\n",
      "\n",
      "EVENT data using CAUSE model\n",
      " Accuracy  0.85\n",
      "F1 score (base)  0.889\n",
      "F1 score (micro) 0.85\n",
      "F1 score (macro) 0.829 \n",
      "\n",
      "\n",
      "\n",
      "CAUSE data using CAUSE model\n",
      " Accuracy  0.786\n",
      "F1 score (base)  0.819\n",
      "F1 score (micro) 0.786\n",
      "F1 score (macro) 0.778 \n",
      "\n",
      "\n",
      "\n",
      "CAUSE data using EVENT model\n",
      " Accuracy  0.779\n",
      "F1 score (base)  0.807\n",
      "F1 score (micro) 0.779\n",
      "F1 score (macro) 0.773 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# # uncomment based on if you want to do EVENT, CAUSE, or, ACTION\n",
    "training_label = 'EVENT'\n",
    "data_label = 'EVENT'\n",
    "# training_label = 'CAUSE'\n",
    "# training_label = 'ACTION'\n",
    "\n",
    "print('EVENT data using EVENT model\\n', HRAF_inference(data_label='EVENT', training_label='EVENT'),'\\n\\n\\n')\n",
    "print('EVENT data using CAUSE model\\n', HRAF_inference(data_label='EVENT', training_label='CAUSE'),'\\n\\n\\n')\n",
    "print('CAUSE data using CAUSE model\\n', HRAF_inference(data_label='CAUSE', training_label='CAUSE'),'\\n\\n\\n')\n",
    "print('CAUSE data using EVENT model\\n', HRAF_inference(data_label='CAUSE', training_label='EVENT'),'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment based on if you want to do EVENT, CAUSE, or, ACTION\n",
    "training_label = 'EVENT'\n",
    "data_label = 'EVENT'\n",
    "# training_label = 'CAUSE'\n",
    "# training_label = 'ACTION'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
