{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CHANGE\n",
    "# replace checkpoint with intended checkpoint containing tokens Id's (only necessary if you are using nltk tokenizer, otherwise, do not worry about this and skip \"Create tokenizer key/value\" section)\n",
    "checkpoint = 'checkpoint-700'\n",
    "# replace model with the name of the model wanted to use\n",
    "model = \"ThreeLargeClasses\"\n",
    "# Edit path as you see fit to reach location where all files associated with the model's output are\n",
    "directory = f\"../HRAF_NLP/HRAF_MultiLabel_{model}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams From Passage Text.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we:\n",
    "- Load passages used within the training model (not input ids)\n",
    "- Remove all punctuation escept for .!?\n",
    "- Lowercase everything.\n",
    "- Split passage into tokens.\n",
    "- Optionally delete all stopwords \n",
    "- Create N-grams.\n",
    "- Optionally remove N-grams with stopwords (either at all or only at the ends) if stopwords have not already been removed\n",
    "- Got frequency of negative and positive coding\n",
    "- Removed N-Grams fewer than 5.\n",
    "- Saved data to Excel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up (run all)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tokenizer key/value (Only relevant if you do not want to use my tokenizer and prefer NLTK tokenizer instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JSON Tokenizer (note, you may need to change this if you do not follow the naming convention I am using. ultimately, this code should work with single label ngrams!)\n",
    "with open(f\"{directory}/HRAF_Model_MultiLabel_{model}/{checkpoint}/tokenizer.json\") as f:\n",
    "    tokenizer_base = json.load(f)\n",
    "\n",
    "# extract only the vocab\n",
    "tokenizer_base = tokenizer_base['model']['vocab']\n",
    "# switch key and values\n",
    "tokenizer = {val: key for key, val in tokenizer_base.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model dataset as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Tokenized passages and inputs\n",
    "with open(f\"{directory}/Datasets/tokenized_inputs.json\") as f:\n",
    "    Hraf = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngram Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import copy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from operator import itemgetter\n",
    "import time\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "def N_gram_creator(text:list, ngram:int, delete_stopwords:bool=False, use_end_tokens:bool=False, fuse_ngrams:bool=True):\n",
    "\n",
    "    # whether or not to include end tokens inside the datasets\n",
    "    if use_end_tokens == True:\n",
    "        start = text.count(\"[CLS]\") + 1\n",
    "        end = text.count(\"[SEP]\") + 1\n",
    "        assert start == end, \"Unequal number of start and end tokens\"\n",
    "        if start < ngram:\n",
    "            text = ['[CLS]'] * (ngram- start) + text + ['[SEP]'] * (ngram- end)\n",
    "            start = text.count(\"[CLS]\") + 1\n",
    "            end = text.count(\"[SEP]\") + 1\n",
    "        temp=zip(*[text[i+(start-ngram):(len(text)-(end-ngram))] for i in range(0,ngram)]) #zip a set of \"n\" Ngrams. disregard \"[sep]\" depending on the NGram number.\n",
    "    else:\n",
    "        # create Ngram by zipping\n",
    "        temp=zip(*[text[i:] for i in range(0,ngram)])\n",
    "    ans=[list(ngram) for ngram in temp]\n",
    "\n",
    "    # delete all ngrams that contain stopwords (after the fact creation of Ngrams)\n",
    "    if delete_stopwords == True:\n",
    "        ngram_buffer = []\n",
    "        cached_stopwords = set(stopwords.words('english'))\n",
    "        for ngram in ans:\n",
    "            if not bool(cached_stopwords.intersection(ngram)):\n",
    "                ngram_buffer.append(ngram)\n",
    "        ans = ngram_buffer\n",
    "    # delete all ngrams that start or end with stop words (per https://stats.stackexchange.com/questions/570698/should-i-remove-stopwords-before-generating-n-grams)\n",
    "    elif delete_stopwords == 'ends':\n",
    "        ngram_buffer = []\n",
    "        for ngram in ans:\n",
    "            if (ngram[0] not in set(stopwords.words('english')) and ngram[-1] not in set(stopwords.words('english'))):\n",
    "                ngram_buffer.append(ngram)\n",
    "        ans = ngram_buffer\n",
    "\n",
    "    # Optionally turn to string\n",
    "    if fuse_ngrams:\n",
    "        ans = [' '.join(ngram) for ngram in ans]\n",
    "    return ans\n",
    "\n",
    "def N_gram_dictionary(passage_dict_list_copy, ngram_num:int, label:str, passagefreq:bool=True, use_end_tokens:bool=False, use_tokenized:bool=False, tokenizer=False, text_name:str='passage', id_name:str='ID', predLabel_name:str='pred_labels', actualLabel_name:str='actual_labels'): #must be a dictionary containing passages as an input\n",
    "    \"\"\"\n",
    "    Create a dataframe composed of specified Ngram and label\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    passage_dict_list_copy : list of dictionaries containing text for Ngrams, predicted labels, and actual labels. Typically derived from machine learning model\n",
    "    ngram_num : int, ngram number\n",
    "    label : string, label used for Ngram\n",
    "    passagefreq : boolean, should it count the frequency of passages occuring?\n",
    "    use_end_tokens : boolean, should we include end tokens like [CLS] as ngrams?\n",
    "    use_tokenized : boolean, is the dataframe already tokenized\n",
    "    tokenizer : NLP tokenizer, you must supply what that tokenizer is!\n",
    "    text_name : string, column name of text used for Ngrams\n",
    "    id_name : string, column name of IDs\n",
    "    predLabel_name : string, column name of predicted label\n",
    "    actualLabel_name : string, column name of actual label\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    Ngram_dict : list of dictionaries containing Ngrams and their frequencies\n",
    "    \"\"\"\n",
    "    \n",
    "    if use_tokenized is True:\n",
    "        assert tokenizer is not False, \"Must supply a tokenizer\"\n",
    "\n",
    "    Ngram_dict = dict()\n",
    "    passage_dict_list = copy.deepcopy(passage_dict_list_copy)\n",
    "    # get Ngrams and assign frequencies\n",
    "    for passage_dict in passage_dict_list:\n",
    "        # return tokenized (and cleaned) passage\n",
    "        if use_tokenized is True:\n",
    "            passage_dict['t_words'] = list(itemgetter(*passage_dict['input_ids'])(tokenizer))\n",
    "            passage_dict['t_words'] = [passage_dict['t_words'][0]] + passage_dict['t_words'] + [passage_dict['t_words'][-1]]\n",
    "        else:\n",
    "            passage_dict['t_words'] = tokenize_words(passage_dict[text_name])\n",
    "\n",
    "        Ngram_passage_count = set() # refresh set for checking if an Ngram has appeared in a passage\n",
    "        # Create NGrams and assign frequencies\n",
    "        for word in N_gram_creator(passage_dict['t_words'], ngram_num, use_end_tokens):\n",
    "            # set up the dictionary for that word \n",
    "            # (frequency is the number of times the Ngram has appeared in total, _pred refers to the model prediction of negative or positive, _actual refers to the RA label;\n",
    "            # percentage is positive count divided by frequency, passage_frequency refers to the number of passages the Ngram has appeared where duplicates in a passage are not counted)\n",
    "            if word not in Ngram_dict.keys():\n",
    "                if passagefreq is True: # count passage frequency\n",
    "                    Ngram_dict[word] = {'Frequency':0, 'Neg_pred':0,  'Neg_actual':0, \"Pos_pred\":0, \"Pos_actual\":0, \"Percentage_pred\":0, \"Percentage_actual\":0, \"Passage_freq\":0, \"passage_ID\":set()}\n",
    "                else: # don't count passage frequency\n",
    "                    Ngram_dict[word] = {'Frequency':0, 'Neg_pred':0,  'Neg_actual':0, \"Pos_pred\":0, \"Pos_actual\":0, \"Percentage_pred\":0, \"Percentage_actual\":0} \n",
    "            Ngram_dict[word]['Frequency'] += 1\n",
    "\n",
    "            # assign frequency if the Ngram has not already appeared in the passage.\n",
    "            if passagefreq is True and word not in Ngram_passage_count:\n",
    "                Ngram_passage_count.add(word)\n",
    "                Ngram_dict[word]['Passage_freq'] += 1\n",
    "                Ngram_dict[word]['passage_ID'].add(passage_dict[id_name])\n",
    "\n",
    "\n",
    "            # Get predicted count\n",
    "            if passage_dict[predLabel_name][label] == 0:\n",
    "                Ngram_dict[word]['Neg_pred'] += 1\n",
    "            elif passage_dict[predLabel_name][label] == 1:\n",
    "                Ngram_dict[word]['Pos_pred'] += 1\n",
    "            else:\n",
    "                raise ValueError\n",
    "            \n",
    "            # Get actual count\n",
    "            if passage_dict[actualLabel_name][label] == 0:\n",
    "                Ngram_dict[word]['Neg_actual'] += 1\n",
    "            elif passage_dict[actualLabel_name][label] == 1:\n",
    "                Ngram_dict[word]['Pos_actual'] += 1\n",
    "            else:\n",
    "                raise ValueError\n",
    "    # assign percentage (positves/total)\n",
    "    for Ngram in Ngram_dict.keys():\n",
    "        Ngram_dict[Ngram]['Percentage_pred'] = Ngram_dict[Ngram]['Pos_pred']/  Ngram_dict[Ngram]['Frequency']\n",
    "        Ngram_dict[Ngram]['Percentage_actual'] = Ngram_dict[Ngram]['Pos_actual']/  Ngram_dict[Ngram]['Frequency']\n",
    "    \n",
    "    return Ngram_dict\n",
    "\n",
    "\n",
    "def tokenize_words(passage:str, removeStopWords=True):\n",
    "    passage = passage.lower() #lower case everything\n",
    "    passage = re.sub(r'\\.\\.\\.', ' ', passage) # replace elipsis with ' '\n",
    "    passage_t = passage.split(\" \") # get tokens (note that nltk.word_tokenize may be a better option but it messes with non-english text too much)\n",
    "    \n",
    "    \n",
    "    # clean up punctuation, remove all but .!? which will become their own token\n",
    "    word_list = []\n",
    "    for word in passage_t:\n",
    "        # optional remove stopwords\n",
    "        punctEnd_list = []\n",
    "        # remove punct from the start of words (some odd punctuation needed to be added).\n",
    "        safety_count = 0 #include safety count to break in case the file runs too long\n",
    "        punct = string.punctuation+'—‘’“”'\n",
    "        while (len(word) > 0) and word[0] in punct:\n",
    "            if word[0] in '?!.':\n",
    "                word_list += word[0]\n",
    "            word = word[1:]\n",
    "            safety_count += 1\n",
    "            assert safety_count<1000\n",
    "        # remove punct from the end of words\n",
    "        while (len(word) > 0) and word[-1] in punct:\n",
    "            if word[-1] in '?!.':\n",
    "                punctEnd_list += word[-1]\n",
    "            word = word[:-1]\n",
    "            safety_count += 1\n",
    "            assert safety_count<1000\n",
    "        # append if there is something to add.\n",
    "        if len(word) > 0:\n",
    "            word_list += [word]\n",
    "        if len(punctEnd_list) >0:\n",
    "            word_list += punctEnd_list\n",
    "    if removeStopWords is True: \n",
    "        cached_stopwords = set(stopwords.words('english'))\n",
    "        word_list = [word for word in word_list if word not in cached_stopwords]\n",
    "    passage_t = word_list\n",
    "    return passage_t\n",
    "\n",
    "# Save the NGram dictionary\n",
    "def saveFile(dictionary, fileName, folder, frequency_cutoff=5):\n",
    "    make_dir(folder)\n",
    "    df2 = pd.DataFrame.from_dict(dictionary, orient='index')\n",
    "    df2.insert(0, 'N-gram', df2.index)\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "    df2 = df2.sort_values(by=['Pos_pred'], ascending=False) \n",
    "    # drop all frequencies 5 or less (to shorten the file)\n",
    "    df2 = df2.loc[df2['Frequency']>=frequency_cutoff]\n",
    "\n",
    "    df2.to_excel(f'{folder}/{fileName}', index=False)\n",
    "    return fileName + ' Complete'\n",
    "\n",
    "# made directory\n",
    "def make_dir(path):\n",
    "    import os\n",
    "    # Check whether the specified path exists or not\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "    # Create a new directory because it does not exist\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only needs \"actual labels\" and text, spits out pandas dataframe for frequency\n",
    "def ngram_frequency_creator(passages:list, label_list:list, ngram_num:int, tokenized_input:bool = False, frequency_cutoff:int=5, percentage_cutoff:float=False):\n",
    "    assert isinstance(passages, list), \"ERROR passage Input must be a list of passages\"\n",
    "    # get tokens and then get ngrams from those tokens\n",
    "    if tokenized_input == False: # does your input need to be tokenized?\n",
    "        assert ~isinstance(passages[0], list), \"ERROR Your Passage list input appears to be a list of lists, are you sure you didn't mean to select tokenized_input = True?\"\n",
    "        tokenizedWords_list = [tokenize_words(passage) for passage in passages]\n",
    "    else:\n",
    "        tokenizedWords_list = passages\n",
    "    NgramTokens_list = [N_gram_creator(tokenizedWords, ngram_num) for tokenizedWords in tokenizedWords_list]\n",
    "    # get dictionary of Ngram frequency and actual label\n",
    "    Ngram_dict = dict()\n",
    "    for index, NgramTokens in enumerate(NgramTokens_list):\n",
    "        for word in NgramTokens:\n",
    "            if word not in Ngram_dict.keys():\n",
    "                Ngram_dict[word] = {'Frequency':0, 'Neg_actual':0, \"Pos_actual\":0, \"Percentage_actual\":0} \n",
    "            Ngram_dict[word]['Frequency'] += 1\n",
    "\n",
    "            # Get actual count\n",
    "            if label_list[index] == 0:\n",
    "                Ngram_dict[word]['Neg_actual'] += 1\n",
    "            elif label_list[index] == 1:\n",
    "                Ngram_dict[word]['Pos_actual'] += 1\n",
    "            else:\n",
    "                raise ValueError\n",
    "    # assign percentage (positves/total)\n",
    "    for Ngram in Ngram_dict.keys():\n",
    "        Ngram_dict[Ngram]['Percentage_actual'] = Ngram_dict[Ngram]['Pos_actual']/  Ngram_dict[Ngram]['Frequency']\n",
    "    \n",
    "    # create dataframe and remove those below cutoff\n",
    "    df2 = pd.DataFrame.from_dict(Ngram_dict, orient='index')\n",
    "    df2.insert(0, 'N-gram', df2.index)\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "\n",
    "    # drop all frequencies that are too small\n",
    "    df2 = df2.loc[df2['Frequency']>=frequency_cutoff]\n",
    "\n",
    "    # delete those with too small frequencies\n",
    "    if percentage_cutoff:\n",
    "        df2 = df2.loc[df2['Percentage_actual']>=percentage_cutoff]\n",
    "    \n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Ngram dictionaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a series of Ngrams from a range of inputs such as NTLK tokenized passages, tokenized passages using this code, and raw passages and corresponding labels <br>\n",
    "Inputs to N_gram_dictionary():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "    passage_dict_list_copy : list of dictionaries containing text for Ngrams, predicted labels, and actual labels. Typically derived from machine learning model\n",
    "    ngram_num : int, ngram number\n",
    "    label : string, label used for Ngram\n",
    "    passagefreq : boolean, should it count the frequency of passages occuring?\n",
    "    use_end_tokens : boolean, should we include end tokens like [CLS] as ngrams?\n",
    "    use_tokenized : boolean, is the dataframe already tokenized\n",
    "    tokenizer : NLP tokenizer, you must supply what that tokenizer is (hint, you might use the toknizer defined at the very top)\n",
    "    text_name : string, column name of text used for Ngrams\n",
    "    id_name : string, column name of IDs\n",
    "    predLabel_name : string, column name of predicted label\n",
    "    actualLabel_name : string, column name of actual label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_EVENT_1-grams.xlsx Complete\n",
      "_EVENT_2-grams.xlsx Complete\n",
      "_EVENT_3-grams.xlsx Complete\n",
      "_CAUSE_1-grams.xlsx Complete\n",
      "_CAUSE_2-grams.xlsx Complete\n",
      "_CAUSE_3-grams.xlsx Complete\n",
      "_ACTION_1-grams.xlsx Complete\n",
      "_ACTION_2-grams.xlsx Complete\n",
      "_ACTION_3-grams.xlsx Complete\n"
     ]
    }
   ],
   "source": [
    "# CHANGE: give a list of N-gram INTEGERS you want to extract\n",
    "Ngram_nums = [1,2,3] \n",
    "# CHANGE: include the suffix at the end of each of these files if you want to keep multiple versions of these files (may be blank)\n",
    "suffix = ''\n",
    "\n",
    "folder = f'{directory}/Ngrams' # Doesn't need changing (will create a folder automatically)\n",
    "\n",
    "\n",
    "for label in Hraf[0]['pred_labels'].keys():\n",
    "    # run through each N-gram\n",
    "    for Ngram_num in Ngram_nums:\n",
    "        file = f'_{label}_{Ngram_num}-grams{suffix}.xlsx'\n",
    "        dictionary = N_gram_dictionary(Hraf, Ngram_num, label, passagefreq=True) # get NGram dictionary\n",
    "        print(saveFile(dictionary=dictionary, fileName=file, folder=folder, frequency_cutoff=5)) # Save NGram dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get simplified Ngrams based on Passage input and label inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is a simplified version of the one above as it cannot accept model tokenized inputs, and has far fewer options. However, its simplicity is to its benefit. This code will output Ngrams with removed stopwords 4 times as fast as the above code and does not require a prexisting model to work. All you need is a list of passages and a list of labels (e.g. 1's and 0's). NOTE if you are wondering whether to use this section or the above section, both will work just fine but the above dictionary code gives more info for the Ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N-gram</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Neg_actual</th>\n",
       "      <th>Pos_actual</th>\n",
       "      <th>Percentage_actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4143</th>\n",
       "      <td>. p .</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30899</th>\n",
       "      <td>. . .</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>0.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40768</th>\n",
       "      <td>cit . p</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>0.826087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84713</th>\n",
       "      <td>. none none</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9745</th>\n",
       "      <td>. reind.-l .</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29289</th>\n",
       "      <td>vol . ii</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28430</th>\n",
       "      <td>. ibid .</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23922</th>\n",
       "      <td>frights collecting souls</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23921</th>\n",
       "      <td>collecting frights collecting</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37375</th>\n",
       "      <td>vol . 2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              N-gram  Frequency  Neg_actual  Pos_actual   \n",
       "4143                           . p .         50           8          42  \\\n",
       "30899                          . . .         40           9          31   \n",
       "40768                        cit . p         23           4          19   \n",
       "84713                    . none none         22          11          11   \n",
       "9745                    . reind.-l .         21           0          21   \n",
       "...                              ...        ...         ...         ...   \n",
       "29289                       vol . ii          5           0           5   \n",
       "28430                       . ibid .          5           0           5   \n",
       "23922       frights collecting souls          5           0           5   \n",
       "23921  collecting frights collecting          5           0           5   \n",
       "37375                        vol . 2          5           0           5   \n",
       "\n",
       "       Percentage_actual  \n",
       "4143            0.840000  \n",
       "30899           0.775000  \n",
       "40768           0.826087  \n",
       "84713           0.500000  \n",
       "9745            1.000000  \n",
       "...                  ...  \n",
       "29289           1.000000  \n",
       "28430           1.000000  \n",
       "23922           1.000000  \n",
       "23921           1.000000  \n",
       "37375           1.000000  \n",
       "\n",
       "[155 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I believe the intention was to have an extremely toned down version (also faster) of the Ngram creater which only needs a list of the inputs\n",
    "passage_list = [line['passage'] for line in Hraf]\n",
    "label_list = [line['actual_labels']['EVENT'] for line in Hraf]\n",
    "\n",
    "\n",
    "df3 = ngram_frequency_creator(passages=passage_list, label_list=label_list, ngram_num=3, frequency_cutoff=5, percentage_cutoff=False)\n",
    "df3.sort_values(by=\"Frequency\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get each Ngram per label based on a list of passages and a list of lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_EVENT_1-grams_alt.xlsx Complete\n",
      "_EVENT_2-grams_alt.xlsx Complete\n",
      "_EVENT_3-grams_alt.xlsx Complete\n",
      "_CAUSE_1-grams_alt.xlsx Complete\n",
      "_CAUSE_2-grams_alt.xlsx Complete\n",
      "_CAUSE_3-grams_alt.xlsx Complete\n",
      "_ACTION_1-grams_alt.xlsx Complete\n",
      "_ACTION_2-grams_alt.xlsx Complete\n",
      "_ACTION_3-grams_alt.xlsx Complete\n"
     ]
    }
   ],
   "source": [
    "### This cell replicates the above ngram dictionary cell.\n",
    "# CHANGE: give a list of N-gram INTEGERS you want to extract\n",
    "Ngram_nums = [1,2,3] \n",
    "# CHANGE: include the suffix at the end of each of these files if you want to keep multiple versions of these files (may be blank)\n",
    "suffix = '_alt'\n",
    "\n",
    "\n",
    "folder = f'{directory}/Ngrams' # Doesn't need changing (will create a folder automatically)\n",
    "\n",
    "labels = Hraf[0]['pred_labels'].keys() # you may change this to whatever but if you do not have a list of lists [[label1], [label2], [label3]]  (i.e. you only have a single label, it is reccommended to still make that label inside a second list: [[label]] )\n",
    "\n",
    "# Get list of passages (then tokenize them to make it faster)\n",
    "passage_list = [line['passage'] for line in Hraf]\n",
    "passage_list_t = [tokenize_words(passage) for passage in passage_list]\n",
    "\n",
    "# Run through each label N-gram pair (Note, the more combinations you have, the more important it is to tokenize your data first)\n",
    "for label in labels:\n",
    "    label_list = [line['actual_labels'][label] for line in Hraf]\n",
    "    # run through each N-gram (WARNING will take much longer than previous N-gram\n",
    "    for Ngram_num in Ngram_nums:\n",
    "        file = f'_{label}_{Ngram_num}-grams{suffix}.xlsx'\n",
    "        df3 = ngram_frequency_creator(passages=passage_list_t, label_list=label_list, ngram_num=Ngram_num, tokenized_input=True, frequency_cutoff=5, percentage_cutoff=False)\n",
    "        df3 = df3.sort_values(by=\"Frequency\", ascending=False)\n",
    "        df3.to_excel(f'{folder}/{file}', index=False)\n",
    "        print(file, \"Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
