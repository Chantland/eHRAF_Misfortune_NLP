{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /Users/ericchantland/.huggingface/token\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "# copy and paste this code in the terminal: huggingface-cli login \n",
    "# then paste this token: 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# It may not show up but still paste the toke in and press enter\n",
    "\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">CULTURE</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"7\" halign=\"left\">ACTION</th>\n",
       "      <th>OTHER</th>\n",
       "      <th colspan=\"2\" halign=\"left\">CODER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Passage Number</th>\n",
       "      <th>Region</th>\n",
       "      <th>SubRegion</th>\n",
       "      <th>Culture</th>\n",
       "      <th>DocTitle</th>\n",
       "      <th>Section</th>\n",
       "      <th>Author</th>\n",
       "      <th>Page</th>\n",
       "      <th>Year</th>\n",
       "      <th>OCM</th>\n",
       "      <th>...</th>\n",
       "      <th>Technical_Specialist</th>\n",
       "      <th>Divination</th>\n",
       "      <th>Shaman_Medium_Healer</th>\n",
       "      <th>Priest_High_Religion</th>\n",
       "      <th>Other</th>\n",
       "      <th>Description</th>\n",
       "      <th>Local_terms</th>\n",
       "      <th>Other_Comments</th>\n",
       "      <th>Finished</th>\n",
       "      <th>Coder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5153</td>\n",
       "      <td>North-America</td>\n",
       "      <td>Plains and Plateau</td>\n",
       "      <td>Blackfoot</td>\n",
       "      <td>Ceremonial bundles of the Blackfoot Indians</td>\n",
       "      <td>( e )</td>\n",
       "      <td>Wissler, Clark, 1870-1947</td>\n",
       "      <td>74</td>\n",
       "      <td>1912</td>\n",
       "      <td>['159', '493', '751', '756', '776', '778']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>narrator recalls seing in a dream that by usin...</td>\n",
       "      <td>0</td>\n",
       "      <td>not sure how to code this one, misfortune is n...</td>\n",
       "      <td>True</td>\n",
       "      <td>AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5206</td>\n",
       "      <td>North-America</td>\n",
       "      <td>Plains and Plateau</td>\n",
       "      <td>Blackfoot</td>\n",
       "      <td>Ceremonial bundles of the Blackfoot Indians</td>\n",
       "      <td>Untitled Section</td>\n",
       "      <td>Wissler, Clark, 1870-1947</td>\n",
       "      <td>88</td>\n",
       "      <td>1912</td>\n",
       "      <td>['177', '436', '750', '756', '778', '832']</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>injury by Cree medicine can be removed by oth...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5208</td>\n",
       "      <td>North-America</td>\n",
       "      <td>Plains and Plateau</td>\n",
       "      <td>Blackfoot</td>\n",
       "      <td>Ceremonial bundles of the Blackfoot Indians</td>\n",
       "      <td>Untitled Section</td>\n",
       "      <td>Wissler, Clark, 1870-1947</td>\n",
       "      <td>90</td>\n",
       "      <td>1912</td>\n",
       "      <td>['177', '609', '753', '755', '761']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>preemptive action: treat all visiting Cree car...</td>\n",
       "      <td>0</td>\n",
       "      <td>not sure if \"treating someone carefully\" is co...</td>\n",
       "      <td>True</td>\n",
       "      <td>AH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CULTURE                                                \\\n",
       "  Passage Number         Region           SubRegion    Culture   \n",
       "1           5153  North-America  Plains and Plateau  Blackfoot   \n",
       "4           5206  North-America  Plains and Plateau  Blackfoot   \n",
       "5           5208  North-America  Plains and Plateau  Blackfoot   \n",
       "\n",
       "                                                                  \\\n",
       "                                      DocTitle           Section   \n",
       "1  Ceremonial bundles of the Blackfoot Indians             ( e )   \n",
       "4  Ceremonial bundles of the Blackfoot Indians  Untitled Section   \n",
       "5  Ceremonial bundles of the Blackfoot Indians  Untitled Section   \n",
       "\n",
       "                                         \\\n",
       "                      Author Page  Year   \n",
       "1  Wissler, Clark, 1870-1947   74  1912   \n",
       "4  Wissler, Clark, 1870-1947   88  1912   \n",
       "5  Wissler, Clark, 1870-1947   90  1912   \n",
       "\n",
       "                                               ...               ACTION  \\\n",
       "                                          OCM  ... Technical_Specialist   \n",
       "1  ['159', '493', '751', '756', '776', '778']  ...                    0   \n",
       "4  ['177', '436', '750', '756', '778', '832']  ...                    1   \n",
       "5         ['177', '609', '753', '755', '761']  ...                    0   \n",
       "\n",
       "                                                              \\\n",
       "  Divination Shaman_Medium_Healer Priest_High_Religion Other   \n",
       "1          0                    0                    0     0   \n",
       "4          0                    0                    0     0   \n",
       "5          0                    0                    0     1   \n",
       "\n",
       "                                                                  \\\n",
       "                                         Description Local_terms   \n",
       "1  narrator recalls seing in a dream that by usin...           0   \n",
       "4   injury by Cree medicine can be removed by oth...           0   \n",
       "5  preemptive action: treat all visiting Cree car...           0   \n",
       "\n",
       "                                               OTHER    CODER        \n",
       "                                      Other_Comments Finished Coder  \n",
       "1  not sure how to code this one, misfortune is n...     True    AH  \n",
       "4                                                  0     True    AH  \n",
       "5  not sure if \"treating someone carefully\" is co...     True    AH  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"../RA_Cleaning/Culture_Coding_old.xlsx\", header=[0,1], index_col=0)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['passage', 'label'],\n",
       "        num_rows: 1400\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['passage', 'label'],\n",
       "        num_rows: 350\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # uncomment based on if you want to do EVENT, CAUSE, or, ACTION\n",
    "# training_label = 'EVENT'\n",
    "training_label = 'CAUSE'\n",
    "# training_label = 'ACTION'\n",
    "\n",
    "\n",
    "# subdivide into just passage and outcome\n",
    "df_small = pd.DataFrame()\n",
    "df_small[[\"passage\",\"label\"]] = df[[('CULTURE', \"Passage\"), (training_label, \"No_Info\")]]\n",
    "# Flip the lable of \"no_info\"\n",
    "df_small[\"label\"] = df_small['label'].replace({0:1, 1:0})\n",
    "\n",
    "\n",
    "# create train and test sets\n",
    "train, test = train_test_split(df_small, test_size=0.2, random_state=10)\n",
    "\n",
    "# Create an NLP friendly dataset\n",
    "Hraf = DatasetDict(\n",
    "    {'train':Dataset.from_dict(train.to_dict(orient= 'list')),\n",
    "     'test':Dataset.from_dict(test.to_dict(orient= 'list'))})\n",
    "Hraf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import collections\n",
    "\n",
    "print(inspect.signature(train_test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As has already been mentioned, on of the subsidiary bird totems of the Malawi clan is katakea, the white cockatoo, whose feathers are used in making a much admired dancing ornament, yet the men of the Malawi clan use its feathers in the same way as other men.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hraf['train'][0]['passage']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load a DistilBERT tokenizer to preprocess the text field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a preprocessing function to tokenize text and truncate sequences to be no longer than DistilBERTâ€™s maximum input length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"passage\"], truncation=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets map function. You can speed up map by setting batched=True to process multiple elements of the dataset at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9abbd5607e0341849a5dc293a4bb6eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83fdb97af84a49269edae11b242c3ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_Hraf = Hraf.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['passage', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1400\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['passage', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 350\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_Hraf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "Percentage Truncated: 7.9%\n",
      "[3, 37, 48, 49, 54, 81, 86, 97, 105, 116, 117, 131, 143, 145, 193, 197, 205, 213, 216, 221, 235, 236, 244, 254, 260, 262, 265, 271, 293, 310, 313, 329, 330, 347, 356, 375, 386, 415, 416, 448, 451, 470, 473, 475, 498, 502, 508, 512, 519, 531, 539, 541, 547, 548, 576, 590, 599, 610, 653, 677, 713, 722, 733, 756, 757, 765, 769, 787, 806, 818, 838, 843, 866, 890, 893, 921, 928, 937, 950, 963, 978, 989, 1008, 1018, 1019, 1032, 1047, 1074, 1081, 1094, 1106, 1112, 1120, 1163, 1181, 1183, 1197, 1211, 1250, 1255, 1256, 1275, 1292, 1297, 1309, 1349, 1351, 1365, 1379, 1381]\n"
     ]
    }
   ],
   "source": [
    "# Number of passages longer than 512 tokens (and therefore truncated)\n",
    "phrase = []\n",
    "sequence_i = []\n",
    "for i, tx in enumerate(tokenized_Hraf['train']):\n",
    "    if len(tx['input_ids']) == 512:\n",
    "        phrase.append(tx['passage'])\n",
    "        sequence_i.append(i)\n",
    "print(len(phrase))\n",
    "print(f'Percentage Truncated: {round(len(phrase)/len(tokenized_Hraf[\"train\"])*100,1)}%')\n",
    "print(sequence_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a batch of examples using <a href=\"https://huggingface.co/docs/transformers/v4.29.0/en/main_classes/data_collator#transformers.DataCollatorWithPadding\"> DataCollatorWithPadding</a>. Itâ€™s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    y_pred, y_true = eval_pred\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    accuracy = accuracy_score(y_pred=y_true, y_true=y_pred)\n",
    "    f1 = f1_score(y_pred=y_true, y_true=y_pred, average='binary') #binary is the base f1, use 'micro' or 'macro' when you have more than 1 class\n",
    "    roc_auc = roc_auc_score(y_true, y_pred) #ROC curve\n",
    "    return  {'accuracy':accuracy, 'f1':f1, 'roc_auc':roc_auc}\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# from datasets import load_metric\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     metric = load_metric('accuracy', 'f1')\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     metric.add_batch(predictions=predictions, references=labels)\n",
    "#     return  metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = (np.array([[.2,.8],[.7,.3],[.45, .55]]), np.array([0,0,1]))\n",
    "# compute_metrics(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Train\n",
    "Before you start training your model, create a map of the expected ids to their labels with id2label and label2id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"ABSENT\", 1: \"PRESENT\"}\n",
    "label2id = {\"ABSENT\": 0, \"PRESENT\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/ericchantland/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ABSENT\",\n",
      "    \"1\": \"PRESENT\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"ABSENT\": 0,\n",
      "    \"PRESENT\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/ericchantland/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: passage. If passage are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/opt/anaconda3/envs/NLP-3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1400\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 352\n",
      "  Number of trainable parameters = 66955010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1188856dc9ad4f13beac0f54fb6cc625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: passage. If passage are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 350\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089f5fc5590e4526a3bddd4a71e4b66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_CAUSE_DEMO/checkpoint-88\n",
      "Configuration saved in HRAF_Model_CAUSE_DEMO/checkpoint-88/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43711233139038086, 'eval_accuracy': 0.8, 'eval_f1': 0.86, 'eval_roc_auc': 0.753509085848344, 'eval_runtime': 125.1322, 'eval_samples_per_second': 2.797, 'eval_steps_per_second': 0.176, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_CAUSE_DEMO/checkpoint-88/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_CAUSE_DEMO/checkpoint-88/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_CAUSE_DEMO/checkpoint-88/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: passage. If passage are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 350\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144418cce9c2484d99d4c26cc9f41080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_CAUSE_DEMO/checkpoint-176\n",
      "Configuration saved in HRAF_Model_CAUSE_DEMO/checkpoint-176/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42503783106803894, 'eval_accuracy': 0.8314285714285714, 'eval_f1': 0.8893058161350844, 'eval_roc_auc': 0.7432303471310986, 'eval_runtime': 110.6957, 'eval_samples_per_second': 3.162, 'eval_steps_per_second': 0.199, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_CAUSE_DEMO/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_CAUSE_DEMO/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_CAUSE_DEMO/checkpoint-176/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: passage. If passage are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 350\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcc79a5964d48e696ca933bf469f8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_CAUSE_DEMO/checkpoint-264\n",
      "Configuration saved in HRAF_Model_CAUSE_DEMO/checkpoint-264/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43897679448127747, 'eval_accuracy': 0.7971428571428572, 'eval_f1': 0.8571428571428571, 'eval_roc_auc': 0.7544435166408208, 'eval_runtime': 108.2072, 'eval_samples_per_second': 3.235, 'eval_steps_per_second': 0.203, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_CAUSE_DEMO/checkpoint-264/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_CAUSE_DEMO/checkpoint-264/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_CAUSE_DEMO/checkpoint-264/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: passage. If passage are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 350\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a444db0b456047f1b4e292adf5c28e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_CAUSE_DEMO/checkpoint-352\n",
      "Configuration saved in HRAF_Model_CAUSE_DEMO/checkpoint-352/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44982752203941345, 'eval_accuracy': 0.8114285714285714, 'eval_f1': 0.87109375, 'eval_roc_auc': 0.7497713626784366, 'eval_runtime': 107.5582, 'eval_samples_per_second': 3.254, 'eval_steps_per_second': 0.205, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_CAUSE_DEMO/checkpoint-352/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_CAUSE_DEMO/checkpoint-352/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_CAUSE_DEMO/checkpoint-352/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from HRAF_Model_CAUSE_DEMO/checkpoint-176 (score: 0.42503783106803894).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6694.3254, 'train_samples_per_second': 0.837, 'train_steps_per_second': 0.053, 'train_loss': 0.36388037421486596, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=352, training_loss=0.36388037421486596, metrics={'train_runtime': 6694.3254, 'train_samples_per_second': 0.837, 'train_steps_per_second': 0.053, 'train_loss': 0.36388037421486596, 'epoch': 4.0})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"HRAF_Model_CAUSE_DEMO\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_Hraf[\"train\"],\n",
    "    eval_dataset=tokenized_Hraf[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: passage. If passage are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 350\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178fe4b91a91496b98b0ecc403a2ef5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.42503783106803894,\n",
       " 'eval_accuracy': 0.8314285714285714,\n",
       " 'eval_f1': 0.8893058161350844,\n",
       " 'eval_roc_auc': 0.7432303471310986,\n",
       " 'eval_runtime': 134.6805,\n",
       " 'eval_samples_per_second': 2.599,\n",
       " 'eval_steps_per_second': 0.163,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967\n",
      "249\n",
      "0.7952302631578947\n"
     ]
    }
   ],
   "source": [
    "print(sum(tokenized_Hraf['train']['label']))\n",
    "print(sum(tokenized_Hraf['test']['label']))\n",
    "print(sum(tokenized_Hraf['train']['label']) / (sum(tokenized_Hraf['test']['label']) + sum(tokenized_Hraf['train']['label'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
