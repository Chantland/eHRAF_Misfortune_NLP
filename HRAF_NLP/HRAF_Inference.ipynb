{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "# If below code does not work, copy and paste this code in the terminal: huggingface-cli login \n",
    "# then paste this read token: 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# It may not show up but still paste the token in and press enter\n",
    "\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment based on if you want to do EVENT, CAUSE, or, ACTION\n",
    "training_label = 'EVENT'\n",
    "# training_label = 'CAUSE'\n",
    "# training_label = 'ACTION'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code you should currently use. Just copy and paste the text in here and will will spit out a demo answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# set up the pipeline\n",
    "classifier = pipeline(\"text-classification\", model=f\"Chantland/HRAF_{training_label}_Demo\", use_auth_token=\"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\", tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'ABSENT', 'score': 0.5897043347358704}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample inference ENTER TEXT IN HERE.\n",
    "text = \"What is meant by the flight to the Milky Way? We are told that behind the visible Milky Way there is another Milky Way, and this is the one the sorcerer visits in his dreams. It is like a kuntanka (tjurunga) and the medicine man walks along it. It is beautiful and smooth like the subincision opening. If the subincision opening were not beautiful and smooth the medicine man might stick there and his soul might never go back into his body. (52) Ibid., pp. 69, 70. ‚Äù\"\n",
    "tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}\n",
    "\n",
    "classifier(text, **tokenizer_kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datset <font color=\"red\">(do not run more than 1 cell)</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old Dataset (dataset this was trained on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['passage', 'label'],\n",
       "    num_rows: 1750\n",
       "})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# load the dataset (THIS IS WHERE YOU WOULD ENTER IN THE DATA YOU WANTED TO TEST!)\n",
    "df = pd.read_excel(\"../RA_Cleaning/Culture_Coding_old.xlsx\", header=[0,1], index_col=0)\n",
    "# subdivide into just passage and outcome\n",
    "df_small = pd.DataFrame()\n",
    "df_small[[\"passage\",\"label\"]] = df[[('CULTURE', \"Passage\"), (training_label, \"No_Info\")]]\n",
    "# Flip the lable of \"no_info\"\n",
    "df_small[\"label\"] = df_small['label'].replace({0:1, 1:0})\n",
    "\n",
    "\n",
    "\n",
    "# Create an NLP friendly dataset\n",
    "Hraf = Dataset.from_dict(df_small.to_dict(orient= 'list'))\n",
    "Hraf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Codings: (Current Dataset - MINUS - Old Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['passage', 'label'],\n",
       "    num_rows: 140\n",
       "})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load OLD and new datasets (THIS IS WHERE YOU WOULD ENTER IN THE DATA YOU WANTED TO TEST!)\n",
    "df_old = pd.read_excel(\"../RA_Cleaning/Culture_Coding_old.xlsx\", header=[0,1], index_col=0)\n",
    "df_current = pd.read_excel(\"../RA_Cleaning/Culture_Coding.xlsx\", header=[0,1], index_col=0)\n",
    "\n",
    "# Remove the runs not of the first from the datasets (this will be made superfluous later but nonetheless is an extra assuredness step)\n",
    "# df_old = df_old.loc[df_old[(\"CODER\",\"Run_Number\")]==1] #if this had \"Run_Number\" column, you would uncomment and run this line\n",
    "df_current = df_current.loc[df_current[(\"CODER\",\"Run_Number\")]==1]\n",
    "\n",
    "# only get new rows that have NOT been trained/tested on before\n",
    "df_new = pd.concat([df_current, df_old])\n",
    "df_new = df_new[~df_new.duplicated(subset=(\"CULTURE\",\"Passage Number\"), keep=False)]\n",
    "\n",
    "\n",
    "# subdivide into just passage and outcome\n",
    "df_small = pd.DataFrame()\n",
    "df_small[[\"passage\",\"label\"]] = df_new[[('CULTURE', \"Passage\"), (training_label, \"No_Info\")]]\n",
    "# Flip the lable of \"no_info\"\n",
    "df_small[\"label\"] = df_small['label'].replace({0:1, 1:0})\n",
    "\n",
    "\n",
    "\n",
    "# Create an NLP friendly dataset\n",
    "Hraf = Dataset.from_dict(df_small.to_dict(orient= 'list'))\n",
    "Hraf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_small_CAUSE =  df_small.copy()\n",
    "# sum(df_small['label'] != df_small_CAUSE['label'])\n",
    "# df_small_CAUSE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of passages and predict them (will take about .25 seconds per passage for me so beware the wait)\n",
    "\n",
    "HrafOutput = []\n",
    "for text in Hraf:\n",
    "    # text = Hraf[text]\n",
    "    score = classifier(text['passage'], **tokenizer_kwargs)\n",
    "    score[0][\"actual_label\"] = text['label']\n",
    "    score[0][\"passage\"] = text['passage']\n",
    "\n",
    "    # change the predicted label into a digit that we can compare\n",
    "    if score[0]['label'] == 'PRESENT':\n",
    "        score[0]['label'] = 1\n",
    "    elif score[0]['label'] == 'ABSENT':\n",
    "        score[0]['label'] = 0\n",
    "    else:\n",
    "        score[0]['label'] = 9\n",
    "\n",
    "\n",
    "    # score[0][(\"actual_label\", 'passage')] = text['passage'], text['label']\n",
    "    HrafOutput.append(score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HrafOutput"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate \"Correctness\" Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.779\n",
      "F1 score (base)  0.807\n",
      "F1 score (micro) 0.779\n",
      "F1 score (macro) 0.773\n"
     ]
    }
   ],
   "source": [
    "# Try preconstructed evaluations\n",
    "\n",
    "# loading precision\n",
    "precision = evaluate.load('precision')\n",
    "result = precision.compute(predictions=[x['label'] for x in HrafOutput], references=[x['actual_label'] for x in HrafOutput])\n",
    "# print(result)\n",
    "\n",
    "# f1 score\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "accuracy = round(accuracy_score([x['actual_label'] for x in HrafOutput], [x['label'] for x in HrafOutput]),3)\n",
    "f1 = round(f1_score([x['actual_label'] for x in HrafOutput], [x['label'] for x in HrafOutput], average = 'binary'),3)\n",
    "f1_micro = round(f1_score([x['actual_label'] for x in HrafOutput], [x['label'] for x in HrafOutput], average='micro'),3)\n",
    "f1_macro = round(f1_score([x['actual_label'] for x in HrafOutput], [x['label'] for x in HrafOutput], average='macro'),3)\n",
    "print(f'Accuracy  {accuracy}\\nF1 score (base)  {f1}\\nF1 score (micro) {f1_micro}\\nF1 score (macro) {f1_macro}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Correctness Metric Myself\n",
    "This was made before finding out we could just load preconstructed, but it is great for the confusion matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up confusion matrix for calculating precision myself\n",
    "confusionMatrix_dict = {\"TruePos\":0, \"FalsePos\":0, \"FalseNeg\":0, \"TrueNeg\":0}\n",
    "for text in HrafOutput:\n",
    "    if text['actual_label'] == 1:\n",
    "        if text['label'] == 1:\n",
    "            confusionMatrix_dict[\"TruePos\"] += 1\n",
    "        elif text['label'] == 0:\n",
    "            confusionMatrix_dict[\"FalseNeg\"] += 1\n",
    "        else:\n",
    "            raise Exception(\"ERROR pos\")\n",
    "    elif text['actual_label'] == 0:\n",
    "        if text['label'] == 1:\n",
    "            confusionMatrix_dict[\"FalsePos\"] += 1\n",
    "        elif text['label'] == 0:\n",
    "            confusionMatrix_dict[\"TrueNeg\"] += 1\n",
    "        else:\n",
    "            raise Exception(\"ERROR neg\")\n",
    "    else:\n",
    "        raise Exception(\"ERROR actual\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TruePos': 68, 'FalsePos': 28, 'FalseNeg': 2, 'TrueNeg': 42}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusionMatrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusionMatrix_dict['TruePos']\n",
    "TN = confusionMatrix_dict['TrueNeg']\n",
    "FP = confusionMatrix_dict['FalsePos']\n",
    "FN = confusionMatrix_dict['FalseNeg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:   0.708\n",
      "Recall:      0.971\n",
      "F1:          0.819\n",
      "MCC:         0.728\n"
     ]
    }
   ],
   "source": [
    "# get scores \n",
    "precision = TP/ (TP + FP)\n",
    "recall = TP/ (TP + FN)\n",
    "F1 = (2 * precision * recall) / (precision + recall)\n",
    "F1_test = TP/ (TP + .5*(FP + FN)) #done to double check work\n",
    "MCC_num = (TP * TN) - (FP - FN)\n",
    "MCC_denom = np.sqrt((TP + FN) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "MCC = MCC_num / MCC_denom #mathews correlation coefficient\n",
    "\n",
    "assert round(F1,5) == round(F1_test,5), ValueError\n",
    "print(f'Precision:   {round(precision,3)}\\nRecall:      {round(recall,3)}\\nF1:          {round(F1,3)}\\nMCC:         {round(MCC,3)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional File save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526960e047c44b0d8ab3bd180d5f6047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# optionally save the file to json\n",
    "from transformers import AutoTokenizer\n",
    "import copy\n",
    "\n",
    "HrafOutput_dummy = copy.deepcopy(HrafOutput)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"passage\"], truncation=True)\n",
    "\n",
    "tokenized_Hraf = Hraf.map(preprocess_function, batched=True)\n",
    "\n",
    "for index, passage in enumerate(HrafOutput_dummy):\n",
    "    assert passage['passage'] == tokenized_Hraf[index]['passage']\n",
    "    passage['predicted_label'] = passage.pop('label') # rename label\n",
    "    passage['actual_label'] = tokenized_Hraf[index]['label']\n",
    "    passage['input_ids'] = tokenized_Hraf[index]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Save to unformatted json (uncomment)\n",
    "with open(f\"../Tokenized_Datasets/tokenized_Hraf_{training_label}.json\", \"w\") as outfile:\n",
    "    json.dump(HrafOutput_dummy, outfile)\n",
    "\n",
    "\n",
    "# # Save to Dataset (uncomment)\n",
    "# HrafOutput_dummy_dataset = Dataset.from_list(HrafOutput_dummy)\n",
    "# Dataset.to_json(HrafOutput_dummy_dataset, f\"../Tokenized_Datasets/tokenized_Hraf_{training_label}_Dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHi Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>CAUSE</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACTION</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>829</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>387</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "CAUSE     0    1\n",
       "ACTION          \n",
       "0       829  361\n",
       "1       387  173"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "ct_EVENT_CAUSE = pd.crosstab(df[('ACTION','No_Info')], df[('CAUSE','No_Info')], rownames=['ACTION'], colnames=['CAUSE'])\n",
    "ct_EVENT_CAUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT by CAUSE:\n",
      "chi: 292.4\n",
      "p:   0.0\n",
      "\n",
      "\n",
      "EVENT by ACTION:\n",
      "chi: 103.3\n",
      "p:   0.0\n",
      "\n",
      "\n",
      "ACTION by CAUSE:\n",
      "chi: 0.0\n",
      "p:   0.857\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chi_square_calc(row, col):\n",
    "    cross_tab = pd.crosstab(df[(row,'No_Info')], df[(col,'No_Info')], rownames=[row], colnames=[col])\n",
    "    stat, p, dof, expected = chi2_contingency(cross_tab)\n",
    "    results = f\"{row} by {col}:\\nchi: {round(stat,1)}\\np:   {round(p,3)}\\n\\n\"\n",
    "    return results\n",
    "\n",
    "group_list = [('EVENT', 'CAUSE'), ('EVENT', 'ACTION'), ('ACTION', 'CAUSE')]\n",
    "for row, col in group_list:\n",
    "    print(chi_square_calc(row, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
