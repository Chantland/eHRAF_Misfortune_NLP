{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color=\"red\">NOTE</font> This code is outdated and a new model shall be created. Culture_Coding.xlsx files have been changed to _Altogether_Dataset_RACoded.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e35182bde84894bec8b46e06cc226d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "# If below code does not work, copy and paste this code in the terminal: huggingface-cli login \n",
    "# then paste this read token: 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# It may not show up but still paste the token in and press enter\n",
    "\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# CHANGE Model name\n",
    "model = \"MultiLabel_ThreeLargeClasses_kfoldsDEMO\"\n",
    "\n",
    "# set up the pipeline from local\n",
    "import os\n",
    "path =os.path.abspath(f\"HRAF_Model_{model}\")\n",
    "classifier = pipeline(\"text-classification\", model=path, top_k=None)\n",
    "\n",
    "# # Demo other models (COMMENT THIS OUT UNLESS YOU REALLY WANT TO DEMO THIS)\n",
    "# # Set up path from online hub (note, this is analogous but different model and is here because this is a demo)\n",
    "# classifier = pipeline(\"text-classification\", top_k=None, model=\"Chantland/Hraf_MultiLabel\", use_auth_token=\"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\", tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\"))\n",
    "# model = \"MultiLabel_ThreeLargeClasses\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code you should currently use. Just copy and paste the text in here and will will spit out a demo answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'EVENT', 'score': 0.9999905824661255},\n",
       "  {'label': 'ACTION', 'score': 0.9999289512634277},\n",
       "  {'label': 'CAUSE', 'score': 1.0256614586978685e-05}]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample inference ENTER TEXT IN HERE.\n",
    "text = '''\n",
    "“Drinking-tubes made of the leg-bones of swans (Fig. 109) are 190 also used chiefly as a measure of precaution against diseases ‘subject to shunning.’....”\n",
    "'''\n",
    "\n",
    "tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}\n",
    "# reveal\n",
    "prediction = classifier(text, **tokenizer_kwargs)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ['EVENT', 'ACTION']\n"
     ]
    }
   ],
   "source": [
    "valid = [label['label'] for label in prediction[0] if label['score']>.5]\n",
    "prediction[0][0]['label']\n",
    "print(\"Prediction:\",valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'passage', 'EVENT', 'CAUSE', 'ACTION'],\n",
       "    num_rows: 1820\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "loc = \"\"\n",
    "# loc = \"../HRAF_MultiLabel_ThreeLargeClasses/\" #load old threemain class (comment this out unless you specifically are using it)\n",
    "\n",
    "f = open(loc+\"Datasets/test_dataset.json\")\n",
    "# f = open(\"../HRAF_MultiLabel_ThreeLargeClasses/Datasets/test_dataset.json\") #load old threemain class (comment this out unless you specifically are using it)\n",
    "data = json.load(f)\n",
    "Hraf = Dataset.from_dict(data)\n",
    "Hraf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EVENT', 'CAUSE', 'ACTION']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get label names\n",
    "labels = [label for label in Hraf.features.keys() if label not in ['ID', 'passage']]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of passages and predict them (will take about .25 seconds per passage for me so beware the wait)\n",
    "\n",
    "HrafOutput = []\n",
    "for text in Hraf:\n",
    "    # get actual labels\n",
    "    actual_labels = [text[label] for label in labels]\n",
    "    prediction = classifier(text['passage'], **tokenizer_kwargs)\n",
    "\n",
    "    # get predicted labels\n",
    "    scores = {item['label']:item['score'] for item in prediction[0]} #turn prediction into a dictionary\n",
    "    pred_labels = [1 if scores[label] >= 0.5 else 0 for label in labels]\n",
    "\n",
    "    \n",
    "    output_dict = dict()\n",
    "    output_dict[\"pred_labels\"] = pred_labels\n",
    "    output_dict[\"actual_labels\"] = actual_labels\n",
    "    output_dict[\"passage\"] = text['passage']\n",
    "    output_dict[\"ID\"] = text['ID']\n",
    "\n",
    "\n",
    "    # score[0][(\"actual_label\", 'passage')] = text['passage'], text['label']\n",
    "    HrafOutput.append(output_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate \"Correctness\" Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT:  0.907\n",
      "CAUSE:  0.822\n",
      "ACTION: 0.805\n",
      "\n",
      "\n",
      "F1 score (micro) 0.851\n",
      "F1 score (macro) 0.845\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "df_score = pd.DataFrame(index=['NLP'], columns= [label+\"_F1\" for label in labels] + [\"Micro_F1\", \"Macro_F1\"])\n",
    "actual_labels = [x['actual_labels'] for x in HrafOutput]\n",
    "pred_labels = [x['pred_labels'] for x in HrafOutput]\n",
    "for index, label in enumerate(labels):\n",
    "    f1 = round(f1_score(y_true=np.array(actual_labels)[:,index], y_pred=np.array(pred_labels)[:,index]),3)\n",
    "    df_score.at['NLP', label+\"_F1\"] = f1\n",
    "    print(f\"{label}: {(6 - len(label)) *' '}{f1}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "f1_micro = round(f1_score(y_true=actual_labels, y_pred=pred_labels, average='micro'),3)\n",
    "f1_macro = round(f1_score(y_true=actual_labels, y_pred=pred_labels, average='macro'),3)\n",
    "df_score.at['NLP', \"Micro_F1\"] = f1_micro\n",
    "df_score.at['NLP', \"Macro_F1\"] = f1_macro\n",
    "print(f'F1 score (micro) {f1_micro}\\nF1 score (macro) {f1_macro}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>EVENT_F1</th>\n",
       "      <th>CAUSE_F1</th>\n",
       "      <th>ACTION_F1</th>\n",
       "      <th>Micro_F1</th>\n",
       "      <th>Macro_F1</th>\n",
       "      <th>test_length</th>\n",
       "      <th>train_length</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>2024-04-04</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.845</td>\n",
       "      <td>1820</td>\n",
       "      <td>7277</td>\n",
       "      <td>model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>2024-02-22</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1085</td>\n",
       "      <td>4340</td>\n",
       "      <td>model: MultiLabel_ThreeLargeClasses, Dataset: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>2024-02-22</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1085</td>\n",
       "      <td>4340</td>\n",
       "      <td>model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.847</td>\n",
       "      <td>1123</td>\n",
       "      <td>4491</td>\n",
       "      <td>model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>2024-02-15</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.846</td>\n",
       "      <td>291</td>\n",
       "      <td>1648</td>\n",
       "      <td>model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lexical search</th>\n",
       "      <td>2024-02-02</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.553</td>\n",
       "      <td>728</td>\n",
       "      <td>2910</td>\n",
       "      <td>Ngram 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>2024-02-02</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.809</td>\n",
       "      <td>728</td>\n",
       "      <td>2910</td>\n",
       "      <td>model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Date EVENT_F1 CAUSE_F1 ACTION_F1 Micro_F1 Macro_F1   \n",
       "NLP            2024-04-04    0.907    0.822     0.805    0.851    0.845  \\\n",
       "NLP            2024-02-22    0.871     0.81     0.793    0.828    0.824   \n",
       "NLP            2024-02-22    0.916    0.827     0.838    0.865     0.86   \n",
       "NLP            2024-02-16    0.906     0.82     0.813    0.851    0.847   \n",
       "NLP            2024-02-15    0.939    0.828      0.77    0.853    0.846   \n",
       "Lexical search 2024-02-02     0.82    0.738     0.656     0.75    0.553   \n",
       "NLP            2024-02-02    0.883    0.812     0.733    0.816    0.809   \n",
       "\n",
       "                test_length  train_length   \n",
       "NLP                    1820          7277  \\\n",
       "NLP                    1085          4340   \n",
       "NLP                    1085          4340   \n",
       "NLP                    1123          4491   \n",
       "NLP                     291          1648   \n",
       "Lexical search          728          2910   \n",
       "NLP                     728          2910   \n",
       "\n",
       "                                                            Notes  \n",
       "NLP             model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...  \n",
       "NLP             model: MultiLabel_ThreeLargeClasses, Dataset: ...  \n",
       "NLP             model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...  \n",
       "NLP             model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...  \n",
       "NLP             model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...  \n",
       "Lexical search                                            Ngram 1  \n",
       "NLP             model: MultiLabel_ThreeLargeClasses_kfoldsDEMO...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# export F1 scores to excel\n",
    "df_scoresSep = df_score.copy()\n",
    "# first load train (and maybe add validation)\n",
    "f = open(loc+\"Datasets/train_dataset.json\")\n",
    "data = json.load(f)\n",
    "train = Dataset.from_dict(data)\n",
    "if os.path.isfile(loc+\"Datasets/validation_dataset.json\"):\n",
    "    f = open(loc+\"Datasets/validation_dataset.json\")\n",
    "    data = json.load(f)\n",
    "    valid = Dataset.from_dict(data)\n",
    "    train = concatenate_datasets([train, valid])\n",
    "# add lengths of test and training set\n",
    "df_scoresSep[[\"test_length\", \"train_length\"]] = (len(Hraf), len(train))\n",
    "# add date\n",
    "df_scoresSep.insert(0, \"Date\", [datetime.today().date()])\n",
    "if loc == \"\":\n",
    "    df_scoresSep['Notes'] = f\"model: {model}, Dataset: {model}\"\n",
    "else:\n",
    "    df_scoresSep['Notes'] = f\"model: {model}, Dataset: {loc}\"\n",
    "# load model_performance.xlsx or else create it\n",
    "if os.path.isfile(\"Model_Prediction_Performance.xlsx\"):\n",
    "    df_oldScores = pd.read_excel(\"Model_Prediction_Performance.xlsx\", index_col=0)\n",
    "    df_oldScores_merged = pd.concat([df_scoresSep, df_oldScores])\n",
    "    nonDateCols = df_oldScores_merged.columns[df_scoresSep.columns != 'Date']\n",
    "    if any(df_oldScores_merged.duplicated(subset=nonDateCols)): # don't append the data unless it is new\n",
    "        print(\"Duplicated scores found, skipping new addition\")\n",
    "        df_scoresSep = df_oldScores.copy()\n",
    "    else:\n",
    "        df_scoresSep = df_oldScores_merged.copy()\n",
    "        df_scoresSep['Date'] = df_scoresSep['Date'].astype('datetime64[ns]')\n",
    "        df_scoresSep.to_excel(\"Model_Prediction_Performance.xlsx\")\n",
    "else:\n",
    "    df_scoresSep['Date'] = df_scoresSep['Date'].astype('datetime64[ns]')\n",
    "    df_scoresSep.to_excel(\"Model_Prediction_Performance.xlsx\")\n",
    "df_scoresSep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional File save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_oldScores_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_oldScores_merged\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_oldScores_merged' is not defined"
     ]
    }
   ],
   "source": [
    "df_oldScores_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HrafOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468d823866d34e7f80f1e3dc120f38b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/728 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# optionally save the file to json\n",
    "from transformers import AutoTokenizer\n",
    "import copy\n",
    "\n",
    "HrafOutput_dummy = copy.deepcopy(HrafOutput)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"passage\"], truncation=True)\n",
    "\n",
    "tokenized_Hraf = Hraf.map(preprocess_function, batched=True)\n",
    "\n",
    "for index, passage in enumerate(HrafOutput_dummy):\n",
    "    assert passage['passage'] == tokenized_Hraf[index]['passage']\n",
    "    passage['pred_labels'] = {key:passage['pred_labels'][index] for index, key in enumerate(labels)}\n",
    "    passage['actual_labels'] = {key:passage['actual_labels'][index] for index, key in enumerate(labels)}\n",
    "    passage['input_ids'] = tokenized_Hraf[index]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Save to unformatted json (uncomment)\n",
    "with open(f\"Datasets/tokenized_inputs.json\", \"w\") as outfile:\n",
    "    json.dump(HrafOutput_dummy, outfile)\n",
    "\n",
    "\n",
    "# # Save to Dataset (uncomment)\n",
    "# HrafOutput_dummy_dataset = Dataset.from_list(HrafOutput_dummy)\n",
    "# Dataset.to_json(HrafOutput_dummy_dataset, f\"Datasets/tokenized_Hraf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHi Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1167,  351],\n",
       "       [  49,  183]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "ct_EVENT_CAUSE = pd.crosstab(df[('EVENT','No_Info')], df[('CAUSE','No_Info')], rownames=['ACTION'], colnames=['CAUSE'])\n",
    "ct_EVENT_CAUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT by CAUSE:\n",
      "chi: 292.4\n",
      "p:   0.0\n",
      "\n",
      "\n",
      "EVENT by ACTION:\n",
      "chi: 103.3\n",
      "p:   0.0\n",
      "\n",
      "\n",
      "ACTION by CAUSE:\n",
      "chi: 0.0\n",
      "p:   0.857\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chi_square_calc(row, col):\n",
    "    cross_tab = pd.crosstab(df[(row,'No_Info')], df[(col,'No_Info')], rownames=[row], colnames=[col])\n",
    "    stat, p, dof, expected = chi2_contingency(cross_tab)\n",
    "    results = f\"{row} by {col}:\\nchi: {round(stat,1)}\\np:   {round(p,3)}\\n\\n\"\n",
    "    return results\n",
    "\n",
    "group_list = [('EVENT', 'CAUSE'), ('EVENT', 'ACTION'), ('ACTION', 'CAUSE')]\n",
    "for row, col in group_list:\n",
    "    print(chi_square_calc(row, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_sqr(obs):\n",
    "    size_x = obs.shape\n",
    "    chi_mat = np.zeros(size_x)\n",
    "    for row in range(size_x[0]):\n",
    "        for col in range(size_x[1]):\n",
    "            exp = np.sum(x[row]) * np.sum(x[:,col]) / np.sum(x)\n",
    "            chi_mat[row, col] = np.sum((obs[row, col] - exp)**2 / exp)\n",
    "    return chi_mat\n",
    "\n",
    "print(np.sum(chi_sqr(x)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
